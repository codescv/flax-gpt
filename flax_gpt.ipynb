{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# FlaxGPT\n",
        "\n",
        "FlaxGPT is a simplistic implementation of GPT (decoder-only transformer) model in [Flax](https://flax.readthedocs.io/en/latest/quick_start.html).\n",
        "\n",
        "The code is minimum in a single notebook and therefore good for hacking and educational purposes."
      ],
      "metadata": {
        "id": "vmUJjJ5qV3Z1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gBmAyj4cPWl",
        "outputId": "1f6735c2-ef82-4a9d-d57a-d5d55fcc033b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m41.0/44.6 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m814.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Library"
      ],
      "metadata": {
        "id": "7iCk1G60Wqp9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NXNz_vAbVwvO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import dataclasses\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "\n",
        "import rich\n",
        "from sentencepiece import SentencePieceProcessor\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp, jax.random as jrandom\n",
        "from flax import linen as nn\n",
        "from flax import traverse_util\n",
        "from flax.training import orbax_utils\n",
        "import orbax.checkpoint\n",
        "import einops\n",
        "\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class Config:\n",
        "  \"\"\"GPT Config.\"\"\"\n",
        "  max_seq_length: int      # maximum context length\n",
        "  vocab_size: int          # vocabulary size\n",
        "  n_embed: int             # embedding size (= n_head * head_size)\n",
        "  n_layer: int             # number of transformer blocks\n",
        "  intermediate_size: int   # intermediate size of FFN\n",
        "\n",
        "  # multi head / multi query attention\n",
        "  n_head: int              # number of heads\n",
        "  n_query_groups: int      # number of query groups in multi-query attention\n",
        "\n",
        "  # RoPE positional embedding\n",
        "  rope_condense_ratio: int # rope condense ratio\n",
        "  rope_base: int           # rope base\n",
        "\n",
        "  def __post_init__(self):\n",
        "    assert self.n_embed % self.n_head == 0, f'Embedding size(n_embed={self.n_embed}) should be divisible by (n_head={self.n_head})'\n",
        "    self.head_size = self.n_embed // self.n_head\n",
        "\n",
        "  @classmethod\n",
        "  def llama2_7b(cls):\n",
        "    return cls(\n",
        "        max_seq_length=4096,\n",
        "        vocab_size=32000,\n",
        "        n_layer=32,\n",
        "        n_head=32,\n",
        "        n_embed=4096,\n",
        "        n_query_groups=32,\n",
        "        intermediate_size=11008,\n",
        "        rope_base=10000,\n",
        "        rope_condense_ratio=1,\n",
        "    )\n",
        "\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "  \"\"\"Root Mean Square Layer Normalization.\n",
        "\n",
        "  Derived from https://github.com/bzhangGo/rmsnorm/blob/master/rmsnorm_torch.py. BSD 3-Clause License:\n",
        "  https://github.com/bzhangGo/rmsnorm/blob/master/LICENSE.\n",
        "  \"\"\"\n",
        "  axis: int = -1\n",
        "  eps: float = 1e-5\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    weight = self.param('weight', lambda rng, shape: jnp.ones(shape), x.shape[-1])\n",
        "    norm_x = jnp.mean(x * x, axis=self.axis, keepdims=True)\n",
        "    x_normed = x / jnp.sqrt(norm_x + self.eps)\n",
        "    return weight * x_normed\n",
        "\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "  \"\"\"Multi head / Multi query / Grouped Query Attention.\n",
        "\n",
        "  About n_query_groups\n",
        "  to use multi-head attention (MHA), set this to `n_head` (default)\n",
        "  to use multi-query attention (MQA), set this to 1\n",
        "  to use grouped-query attention (GQA), set this to a value in between\n",
        "  Example with `n_head=4`\n",
        "  ┌───┐┌───┐┌───┐┌───┐     ┌───┐    ┌───┐             ┌───┐\n",
        "  │ v ││ v ││ v ││ v │     │ v │    │ v │             │ v │\n",
        "  └───┘└───┘└───┘└───┘     └───┘    └───┘             └───┘\n",
        "    │    │    │    │         │        │                 │\n",
        "  ┌───┐┌───┐┌───┐┌───┐     ┌───┐    ┌───┐             ┌───┐\n",
        "  │ k ││ k ││ k ││ k │     │ k │    │ k │             │ k │\n",
        "  └───┘└───┘└───┘└───┘     └───┘    └───┘             └───┘\n",
        "    │    │    │    │      ┌──┴──┐  ┌──┴──┐      ┌────┬──┴─┬────┐\n",
        "  ┌───┐┌───┐┌───┐┌───┐  ┌───┐┌───┐┌───┐┌───┐  ┌───┐┌───┐┌───┐┌───┐\n",
        "  │ q ││ q ││ q ││ q │  │ q ││ q ││ q ││ q │  │ q ││ q ││ q ││ q │\n",
        "  └───┘└───┘└───┘└───┘  └───┘└───┘└───┘└───┘  └───┘└───┘└───┘└───┘\n",
        "  ◀──────────────────▶  ◀──────────────────▶  ◀──────────────────▶\n",
        "          MHA                    GQA                   MQA\n",
        "    n_query_groups=4       n_query_groups=2      n_query_groups=1\n",
        "    q_per_kv=1             q_per_kv=2            q_per_kv=4\n",
        "    n_head=4               n_head=4              n_head=4\n",
        "    n_qkv=3                n_qkv=4               n_qkv=6\n",
        "  credit https://arxiv.org/pdf/2305.13245.pdf\n",
        "  \"\"\"\n",
        "  config: Config\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x, rope_emb, mask=None):\n",
        "    T = x.shape[-2]  # x: (B, T, C)\n",
        "    mask = mask or jnp.tril(jnp.ones((T, T)))\n",
        "\n",
        "    # nq = n_head, nk = nv = n_query_groups\n",
        "    qkv_dim = (self.config.n_head + 2 * self.config.n_query_groups) * self.config.head_size\n",
        "    qkv_proj = nn.Dense(features=qkv_dim, use_bias=False, name='proj_qkv')(x)\n",
        "\n",
        "    # number of q's per group\n",
        "    q_per_kv = self.config.n_head // self.config.n_query_groups\n",
        "    # number of qkvs per group, k=v=1\n",
        "    n_qkv = q_per_kv + 2\n",
        "    # break embedding into (n_groups, n_qkv, head_size)\n",
        "    qkv = einops.rearrange(qkv_proj, 'b t (n_groups n_qkv h) -> b n_groups n_qkv t h',\n",
        "                           n_groups=self.config.n_query_groups,\n",
        "                           n_qkv=n_qkv)\n",
        "    # split q, k, v within groups\n",
        "    q, k, v = einops.unpack(qkv, [[q_per_kv], [1], [1]], 'b n_groups * t h')\n",
        "\n",
        "    if q_per_kv != 1:\n",
        "      # repeat k and v in each group\n",
        "      k = einops.repeat(k, 'b n_groups 1 t h -> b n_groups q_per_kv t h', q_per_kv=q_per_kv)\n",
        "      v = einops.repeat(v, 'b n_groups 1 t h -> b n_groups q_per_kv t h', q_per_kv=q_per_kv)\n",
        "\n",
        "    # merge groups into heads\n",
        "    q = einops.rearrange(q, 'b n_groups q_per_kv t h -> b (n_groups q_per_kv) t h')\n",
        "    k = einops.rearrange(k, 'b n_groups q_per_kv t h -> b (n_groups q_per_kv) t h')\n",
        "    v = einops.rearrange(v, 'b n_groups q_per_kv t h -> b (n_groups q_per_kv) t h')\n",
        "\n",
        "    # apply position embedding\n",
        "    # NOTE: only apply to q and k, but not v\n",
        "    q = apply_rope(q, rope_emb)\n",
        "    k = apply_rope(k, rope_emb)\n",
        "\n",
        "    # multi head scaled dot attention\n",
        "    weights = einops.einsum(q, k, 'b nh tq h, b nh tk h -> b nh tq tk')\n",
        "    weights = weights / jnp.sqrt(self.config.head_size)\n",
        "    weights = jnp.where(mask, weights, float('-inf'))\n",
        "    weights = nn.softmax(weights, axis=-1)\n",
        "    out = einops.einsum(weights, v, 'b nh tq tv, b nh tv h -> b tq nh h')\n",
        "\n",
        "    # concat heads\n",
        "    out = einops.rearrange(out, 'b t nh h -> b t (nh h)')\n",
        "\n",
        "    # final projection\n",
        "    out = nn.Dense(self.config.n_embed, use_bias=False, name='proj_out')(out)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  \"\"\"LLaMA style MLP.\"\"\"\n",
        "  config: Config\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    x1 = nn.Dense(self.config.intermediate_size, use_bias=False, name='fc_1')(x)\n",
        "    x2 = nn.Dense(self.config.intermediate_size, use_bias=False, name='fc_2')(x)\n",
        "    x = nn.silu(x1) * x2\n",
        "    x = nn.Dense(self.config.n_embed, use_bias=False, name='proj_out')(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "  \"\"\"A Transformer Block of attention followed by MLP.\"\"\"\n",
        "  config: Config\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x, rope_emb, mask=None):\n",
        "    n1 = RMSNorm(name='norm_1')(x)\n",
        "    h = SelfAttention(self.config, name='attn')(n1, rope_emb, mask=mask)\n",
        "    x = h + x\n",
        "    n2 = RMSNorm(name='norm_2')(x)\n",
        "    h = MLP(self.config, name='mlp')(n2)\n",
        "    x = h + x\n",
        "    return x\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "  \"\"\"The full decoder only tranformer.\"\"\"\n",
        "\n",
        "  config: Config\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    T = x.shape[-1]  # (B, T)\n",
        "    rope_emb = self.variable('cache', 'rope_emb', build_rope_cache,\n",
        "                             self.config.max_seq_length,\n",
        "                             self.config.head_size,\n",
        "                             self.config.rope_base,\n",
        "                             self.config.rope_condense_ratio)\n",
        "\n",
        "    x = nn.Embed(num_embeddings=self.config.vocab_size,\n",
        "                 features=self.config.n_embed, name='emb')(x)\n",
        "    self.sow('intermediates', 'emb_out', x)\n",
        "\n",
        "    for i in range(self.config.n_layer):\n",
        "      x = Block(config=self.config, name=f'block_{i}')(x, rope_emb.value[:T])\n",
        "      self.sow('intermediates', f'block_{i}_out', x)\n",
        "\n",
        "    # final layer norm\n",
        "    x = RMSNorm(name='ln_f')(x)\n",
        "    self.sow('intermediates', 'ln_out', x)\n",
        "    # language model head\n",
        "    x = nn.Dense(self.config.vocab_size, name='lm_head', use_bias=False)(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "class Tokenizer:\n",
        "  \"\"\"tokenizing and encoding/decoding text using SentencePiece.\n",
        "\n",
        "  Taken from llama codebase: https://github.com/facebookresearch/llama/blob/main/llama/tokenizer.py\n",
        "  \"\"\"\n",
        "  def __init__(self, model_path: str):\n",
        "    # reload tokenizer\n",
        "    assert os.path.isfile(model_path), model_path\n",
        "    self.sp_model = SentencePieceProcessor(model_file=model_path)\n",
        "\n",
        "    # BOS / EOS token IDs\n",
        "    self.n_words: int = self.sp_model.vocab_size()\n",
        "    self.bos_id: int = self.sp_model.bos_id()\n",
        "    self.eos_id: int = self.sp_model.eos_id()\n",
        "    self.pad_id: int = self.sp_model.pad_id()\n",
        "\n",
        "    assert self.sp_model.vocab_size() == self.sp_model.get_piece_size()\n",
        "\n",
        "  def encode(self, s: str, bos: bool, eos: bool) -> List[int]:\n",
        "    assert type(s) is str\n",
        "    t = self.sp_model.encode(s)\n",
        "    if bos:\n",
        "      t = [self.bos_id] + t\n",
        "    if eos:\n",
        "      t = t + [self.eos_id]\n",
        "    return t\n",
        "\n",
        "  def decode(self, t: List[int]) -> str:\n",
        "    return self.sp_model.decode(t)\n",
        "\n",
        "\n",
        "def generate(key, model, tokenizer, variables, prompt, max_tokens=100):\n",
        "  \"\"\"Example implementation for sampling from the model.\"\"\"\n",
        "\n",
        "  x = jnp.array(tokenizer.encode(prompt, bos=True, eos=False)).reshape(1, -1)\n",
        "  result = x[0].tolist()\n",
        "\n",
        "  for t in range(max_tokens):\n",
        "    x = x[..., -model.config.max_seq_length:]\n",
        "    logits = model.apply(variables, x=x)\n",
        "    next_token_logits = logits[:, -1, :]\n",
        "    next_token = jrandom.categorical(jrandom.fold_in(key, t), next_token_logits)\n",
        "    print(f't={t}', tokenizer.decode(result))\n",
        "    if next_token.item() == tokenizer.eos_id:\n",
        "      break\n",
        "    result.append(next_token.item())\n",
        "    x = jnp.concatenate((x, next_token.reshape(-1,1)), axis=-1)\n",
        "\n",
        "  return tokenizer.decode(result)\n",
        "\n",
        "\n",
        "def build_rope_cache(\n",
        "    seq_len: int,\n",
        "    n_elem: int,\n",
        "    base: int = 10000,\n",
        "    condense_ratio: int = 1\n",
        "):\n",
        "  \"\"\"Enhanced Transformer with Rotary Position Embedding.\n",
        "\n",
        "  Derived from: https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/\n",
        "  transformers/rope/__init__.py. MIT License:\n",
        "  https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/license.\n",
        "  \"\"\"\n",
        "  # $\\Theta = {\\theta_i = 10000^{\\frac{2(i-1)}{d}}, i \\in [1, 2, ..., \\frac{d}{2}]}$\n",
        "  theta = 1.0 / (base ** (jnp.arange(0, n_elem, 2) / n_elem))\n",
        "\n",
        "  # Create position indexes `[0, 1, ..., seq_len - 1]`\n",
        "  seq_idx = jnp.arange(seq_len) / condense_ratio\n",
        "\n",
        "  # Calculate the product of position index and $\\theta_i$\n",
        "  idx_theta = jnp.outer(seq_idx, theta)\n",
        "  idx_theta = jnp.tile(idx_theta, (1, 2))\n",
        "\n",
        "  return jnp.stack([jnp.cos(idx_theta), jnp.sin(idx_theta)], axis=-1)\n",
        "\n",
        "\n",
        "def apply_rope(x, rope_emb):\n",
        "  \"\"\"Apply rope embedding to input x.\"\"\"\n",
        "  cos, sin = rope_emb[..., 0], rope_emb[..., 1]\n",
        "  head_size = x.shape[-1]\n",
        "  x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)\n",
        "  x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)\n",
        "  rotated = jnp.concatenate((-x2, x1), axis=-1)  # (B, nh, T, hs)\n",
        "  roped = (x * cos) + (rotated * sin)\n",
        "  return roped\n",
        "\n",
        "\n",
        "def save_checkpoint(variables, path: Path):\n",
        "  \"\"\"Saves model to checkpoint.\"\"\"\n",
        "  orbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
        "  model_index = {}\n",
        "  for name, value in traverse_util.flatten_dict(variables, sep='.').items():\n",
        "    ckpt = {'value': value}\n",
        "    orbax_checkpointer.save(path / name, ckpt, save_args=orbax_utils.save_args_from_target(ckpt))\n",
        "    model_index[name] = 'true'\n",
        "    print(f'Saved {name}')\n",
        "\n",
        "  with open(path / 'model_index.json', 'w') as f:\n",
        "    json.dump(model_index, f)\n",
        "  print(f'Save success')\n",
        "\n",
        "\n",
        "def load_checkpoint(path: Path):\n",
        "  \"\"\"Loads model from checkpoint.\"\"\"\n",
        "  with open(path / 'model_index.json') as f:\n",
        "    model_index = json.load(f)\n",
        "\n",
        "  orbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
        "\n",
        "  variables = {}\n",
        "  for name in model_index:\n",
        "    ckpt = orbax_checkpointer.restore(path / name)\n",
        "    variables[name] = jax.device_put(ckpt['value'])\n",
        "    print(f'Loaded variable: {name}')\n",
        "\n",
        "  return traverse_util.unflatten_dict(variables, sep='.')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils"
      ],
      "metadata": {
        "id": "wuXt3SRcY9oR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Checkpoint Converter\n",
        "# @markdown Download the model and tokenizer files from [hugginface](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/tree/main).\n",
        "# @markdown\n",
        "# @markdown The needed files are\n",
        "# @markdown - `pytorch_model-*.bin`\n",
        "# @markdown - `pytorch_model.bin.index.json`\n",
        "# @markdown - `tokenizer.model`\n",
        "\n",
        "llama_checkpoint = '/content/drive/MyDrive/checkpoints/meta-llama/Llama-2-7b-chat-hf' # @param {type:\"string\"}\n",
        "flaxgpt_checkpoint = '/content/drive/MyDrive/checkpoints/flax-gpt/Llama-2-7b-chat-hf' # @param {type:\"string\"}\n",
        "\n",
        "import re\n",
        "import torch\n",
        "\n",
        "def init_from_llama_checkpoint(\n",
        "    model: GPT,\n",
        "    config: Config,\n",
        "    llama_checkpoint: Path):\n",
        "  variable_shapes = jax.eval_shape(model.init, jrandom.key(0), jnp.zeros((1,1), dtype=jnp.int32))\n",
        "  param_shapes = traverse_util.flatten_dict(variable_shapes['params'], sep='.')\n",
        "\n",
        "  if not llama_checkpoint.is_dir():\n",
        "    raise ValueError(f'llama checkpoint directory {llama_checkpoint} does not exist')\n",
        "\n",
        "  with open(llama_checkpoint / 'pytorch_model.bin.index.json', 'r') as f:\n",
        "    llama_model_index = json.load(f)\n",
        "\n",
        "  param_names = sorted(llama_model_index['weight_map'].keys())\n",
        "  files = sorted(list(set(llama_model_index['weight_map'].values())))\n",
        "\n",
        "  params = {}\n",
        "\n",
        "  def load_params(name, value):\n",
        "    if hasattr(value, 'numpy'):\n",
        "      value = value.numpy()\n",
        "    assert name in param_shapes, f'Param does not exist: {name}'\n",
        "    assert param_shapes[name].shape == value.shape, f'Shapes not match: {name} Expected: {param_shapes[name].shape} Actual: {value.shape}'\n",
        "    params[name] = jax.device_put(value)\n",
        "    print(f'Success: loaded param: {name}, dtype:{params[name].dtype} shape:{params[name].shape} device:{params[name].device()}')\n",
        "\n",
        "  qkv = {i: {} for i in range(config.n_layer)}\n",
        "\n",
        "  for f in files:\n",
        "    print(f'Loading checkpoint file {f}')\n",
        "    states = torch.load(llama_checkpoint / f)\n",
        "    for name, value in states.items():\n",
        "      if name == 'lm_head.weight':\n",
        "        load_params('lm_head.kernel', value.T)\n",
        "      elif name == 'model.embed_tokens.weight':\n",
        "        load_params('emb.embedding', value)\n",
        "      elif name == 'model.norm.weight':\n",
        "        load_params('ln_f.weight', value)\n",
        "      elif ret := re.match(r'model.layers\\.(\\d+)\\.(.*)', name):\n",
        "        i, sub_name = ret.groups()\n",
        "        i = int(i)\n",
        "        if sub_name == 'input_layernorm.weight':\n",
        "          load_params(f'block_{i}.norm_1.weight', value)\n",
        "        elif sub_name == 'post_attention_layernorm.weight':\n",
        "          load_params(f'block_{i}.norm_2.weight', value)\n",
        "        elif sub_name == 'mlp.gate_proj.weight':\n",
        "          load_params(f'block_{i}.mlp.fc_1.kernel', value.T)\n",
        "        elif sub_name == 'mlp.up_proj.weight':\n",
        "          load_params(f'block_{i}.mlp.fc_2.kernel', value.T)\n",
        "        elif sub_name == 'mlp.down_proj.weight':\n",
        "          load_params(f'block_{i}.mlp.proj_out.kernel', value.T)\n",
        "        elif sub_name == 'self_attn.o_proj.weight':\n",
        "          load_params(f'block_{i}.attn.proj_out.kernel', value.T)\n",
        "        elif sub_name == 'self_attn.q_proj.weight':\n",
        "          qkv[i]['q'] = value.numpy()\n",
        "        elif sub_name == 'self_attn.k_proj.weight':\n",
        "          qkv[i]['k'] = value.numpy()\n",
        "        elif sub_name == 'self_attn.v_proj.weight':\n",
        "          qkv[i]['v'] = value.numpy()\n",
        "        elif sub_name == 'self_attn.rotary_emb.inv_freq':\n",
        "          pass\n",
        "        else:\n",
        "          raise ValueError(f'unhandled param: {name}')\n",
        "      else:\n",
        "        raise ValueError(f'unhandled param: {name}')\n",
        "    del(states)  # save memory\n",
        "\n",
        "  def combine_qkv(q, k, v, n_heads):\n",
        "    q = einops.rearrange(q, '(nh h) n_embed -> n_embed nh 1 h', nh=n_heads)\n",
        "    k = einops.rearrange(k, '(nh h) n_embed -> n_embed nh 1 h', nh=n_heads)\n",
        "    v = einops.rearrange(v, '(nh h) n_embed -> n_embed nh 1 h', nh=n_heads)\n",
        "\n",
        "    packed, _ = einops.pack([q,k,v], 'n_embed nh * h')\n",
        "    qkv = einops.rearrange(packed, 'n_embed nh n_qkv h -> n_embed (nh n_qkv h)')\n",
        "    return qkv\n",
        "\n",
        "  for i in range(config.n_layer):\n",
        "    q, k, v = qkv[i]['q'], qkv[i]['k'], qkv[i]['v']\n",
        "    proj_qkv = combine_qkv(q, k, v, config.n_head)\n",
        "    load_params(f'block_{i}.attn.proj_qkv.kernel', proj_qkv)\n",
        "\n",
        "  return traverse_util.unflatten_dict(params, sep='.')\n",
        "\n",
        "\n",
        "llama2_7b_config = Config.llama2_7b()\n",
        "\n",
        "model = GPT(llama2_7b_config)\n",
        "params = init_from_llama_checkpoint(\n",
        "    model, llama2_7b_config, Path(llama_checkpoint))\n",
        "\n",
        "variables = model.init(jrandom.key(0), jnp.zeros((1,1), dtype=jnp.int32))\n",
        "del(variables['params'])\n",
        "variables['params'] = params\n",
        "\n",
        "save_checkpoint(variables, Path(flaxgpt_checkpoint))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "-Ot6pgyFWALE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WYnD5HZTfgpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generation Demo"
      ],
      "metadata": {
        "id": "ZTEMk_-xfhTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.random as jrandom\n",
        "import jax.numpy as jnp\n",
        "\n",
        "tokenizer_model_path = '/content/drive/MyDrive/checkpoints/meta-llama/Llama-2-7b-chat-hf/tokenizer.model'\n",
        "flaxgpt_checkpoint = '/content/drive/MyDrive/checkpoints/flax-gpt/Llama-2-7b-chat-hf'\n",
        "model = GPT(Config.llama2_7b())\n",
        "variables = load_checkpoint(Path(flaxgpt_checkpoint))\n",
        "tokenizer = Tokenizer(tokenizer_model_path)\n",
        "\n",
        "prompt = \"Hello, my name is\"\n",
        "max_tokens = 10\n",
        "key = jrandom.PRNGKey(1337)\n",
        "x = jnp.array(tokenizer.encode(prompt, bos=True, eos=False)).reshape(1, -1)\n",
        "result = x[0].tolist()\n",
        "\n",
        "for t in range(max_tokens):\n",
        "  x = x[..., -model.config.max_seq_length:]\n",
        "  logits = model.apply(variables, x=x)\n",
        "  next_token_logits = logits[:, -1, :]\n",
        "  next_token = jrandom.categorical(jrandom.fold_in(key, t), next_token_logits)\n",
        "  print(f't={t}', tokenizer.decode(result))\n",
        "  if next_token.item() == tokenizer.eos_id:\n",
        "    break\n",
        "  result.append(next_token.item())\n",
        "  x = jnp.concatenate((x, next_token.reshape(-1,1)), axis=-1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2wJA2rtfmcO",
        "outputId": "f2be1e0f-e373-47a7-b20e-2dbeb893cc01"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded variable: cache.rope_emb\n",
            "Loaded variable: params.emb.embedding\n",
            "Loaded variable: params.block_0.attn.proj_out.kernel\n",
            "Loaded variable: params.block_0.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_0.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_0.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_0.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_0.norm_1.weight\n",
            "Loaded variable: params.block_0.norm_2.weight\n",
            "Loaded variable: params.block_1.attn.proj_out.kernel\n",
            "Loaded variable: params.block_1.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_1.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_1.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_1.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_1.norm_1.weight\n",
            "Loaded variable: params.block_1.norm_2.weight\n",
            "Loaded variable: params.block_2.attn.proj_out.kernel\n",
            "Loaded variable: params.block_2.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_2.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_2.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_2.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_2.norm_1.weight\n",
            "Loaded variable: params.block_2.norm_2.weight\n",
            "Loaded variable: params.block_3.attn.proj_out.kernel\n",
            "Loaded variable: params.block_3.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_3.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_3.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_3.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_3.norm_1.weight\n",
            "Loaded variable: params.block_3.norm_2.weight\n",
            "Loaded variable: params.block_4.attn.proj_out.kernel\n",
            "Loaded variable: params.block_4.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_4.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_4.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_4.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_4.norm_1.weight\n",
            "Loaded variable: params.block_4.norm_2.weight\n",
            "Loaded variable: params.block_5.attn.proj_out.kernel\n",
            "Loaded variable: params.block_5.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_5.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_5.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_5.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_5.norm_1.weight\n",
            "Loaded variable: params.block_5.norm_2.weight\n",
            "Loaded variable: params.block_6.attn.proj_out.kernel\n",
            "Loaded variable: params.block_6.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_6.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_6.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_6.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_6.norm_1.weight\n",
            "Loaded variable: params.block_6.norm_2.weight\n",
            "Loaded variable: params.block_7.attn.proj_out.kernel\n",
            "Loaded variable: params.block_7.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_7.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_7.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_7.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_7.norm_1.weight\n",
            "Loaded variable: params.block_7.norm_2.weight\n",
            "Loaded variable: params.block_8.attn.proj_out.kernel\n",
            "Loaded variable: params.block_8.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_8.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_8.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_8.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_8.norm_1.weight\n",
            "Loaded variable: params.block_8.norm_2.weight\n",
            "Loaded variable: params.block_9.attn.proj_out.kernel\n",
            "Loaded variable: params.block_9.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_9.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_9.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_9.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_9.norm_1.weight\n",
            "Loaded variable: params.block_9.norm_2.weight\n",
            "Loaded variable: params.block_10.attn.proj_out.kernel\n",
            "Loaded variable: params.block_10.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_10.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_10.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_10.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_10.norm_1.weight\n",
            "Loaded variable: params.block_10.norm_2.weight\n",
            "Loaded variable: params.block_11.attn.proj_out.kernel\n",
            "Loaded variable: params.block_11.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_11.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_11.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_11.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_11.norm_1.weight\n",
            "Loaded variable: params.block_11.norm_2.weight\n",
            "Loaded variable: params.block_12.attn.proj_out.kernel\n",
            "Loaded variable: params.block_12.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_12.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_12.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_12.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_12.norm_1.weight\n",
            "Loaded variable: params.block_12.norm_2.weight\n",
            "Loaded variable: params.block_13.attn.proj_out.kernel\n",
            "Loaded variable: params.block_13.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_13.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_13.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_13.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_13.norm_1.weight\n",
            "Loaded variable: params.block_13.norm_2.weight\n",
            "Loaded variable: params.block_14.attn.proj_out.kernel\n",
            "Loaded variable: params.block_14.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_14.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_14.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_14.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_14.norm_1.weight\n",
            "Loaded variable: params.block_14.norm_2.weight\n",
            "Loaded variable: params.block_15.attn.proj_out.kernel\n",
            "Loaded variable: params.block_15.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_15.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_15.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_15.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_15.norm_1.weight\n",
            "Loaded variable: params.block_15.norm_2.weight\n",
            "Loaded variable: params.block_16.attn.proj_out.kernel\n",
            "Loaded variable: params.block_16.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_16.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_16.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_16.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_16.norm_1.weight\n",
            "Loaded variable: params.block_16.norm_2.weight\n",
            "Loaded variable: params.block_17.attn.proj_out.kernel\n",
            "Loaded variable: params.block_17.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_17.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_17.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_17.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_17.norm_1.weight\n",
            "Loaded variable: params.block_17.norm_2.weight\n",
            "Loaded variable: params.block_18.attn.proj_out.kernel\n",
            "Loaded variable: params.block_18.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_18.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_18.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_18.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_18.norm_1.weight\n",
            "Loaded variable: params.block_18.norm_2.weight\n",
            "Loaded variable: params.block_19.attn.proj_out.kernel\n",
            "Loaded variable: params.block_19.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_19.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_19.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_19.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_19.norm_1.weight\n",
            "Loaded variable: params.block_19.norm_2.weight\n",
            "Loaded variable: params.block_20.attn.proj_out.kernel\n",
            "Loaded variable: params.block_20.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_20.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_20.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_20.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_20.norm_1.weight\n",
            "Loaded variable: params.block_20.norm_2.weight\n",
            "Loaded variable: params.block_21.attn.proj_out.kernel\n",
            "Loaded variable: params.block_21.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_21.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_21.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_21.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_21.norm_1.weight\n",
            "Loaded variable: params.block_21.norm_2.weight\n",
            "Loaded variable: params.block_22.attn.proj_out.kernel\n",
            "Loaded variable: params.block_22.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_22.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_22.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_22.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_22.norm_1.weight\n",
            "Loaded variable: params.block_22.norm_2.weight\n",
            "Loaded variable: params.block_23.attn.proj_out.kernel\n",
            "Loaded variable: params.block_23.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_23.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_23.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_23.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_23.norm_1.weight\n",
            "Loaded variable: params.block_23.norm_2.weight\n",
            "Loaded variable: params.block_24.attn.proj_out.kernel\n",
            "Loaded variable: params.block_24.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_24.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_24.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_24.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_24.norm_1.weight\n",
            "Loaded variable: params.block_24.norm_2.weight\n",
            "Loaded variable: params.block_25.attn.proj_out.kernel\n",
            "Loaded variable: params.block_25.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_25.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_25.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_25.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_25.norm_1.weight\n",
            "Loaded variable: params.block_25.norm_2.weight\n",
            "Loaded variable: params.block_26.attn.proj_out.kernel\n",
            "Loaded variable: params.block_26.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_26.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_26.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_26.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_26.norm_1.weight\n",
            "Loaded variable: params.block_26.norm_2.weight\n",
            "Loaded variable: params.block_27.attn.proj_out.kernel\n",
            "Loaded variable: params.block_27.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_27.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_27.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_27.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_27.norm_1.weight\n",
            "Loaded variable: params.block_27.norm_2.weight\n",
            "Loaded variable: params.block_28.attn.proj_out.kernel\n",
            "Loaded variable: params.block_28.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_28.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_28.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_28.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_28.norm_1.weight\n",
            "Loaded variable: params.block_28.norm_2.weight\n",
            "Loaded variable: params.block_29.attn.proj_out.kernel\n",
            "Loaded variable: params.block_29.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_29.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_29.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_29.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_29.norm_1.weight\n",
            "Loaded variable: params.block_29.norm_2.weight\n",
            "Loaded variable: params.block_30.attn.proj_out.kernel\n",
            "Loaded variable: params.block_30.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_30.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_30.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_30.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_30.norm_1.weight\n",
            "Loaded variable: params.block_30.norm_2.weight\n",
            "Loaded variable: params.block_31.attn.proj_out.kernel\n",
            "Loaded variable: params.block_31.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_31.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_31.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_31.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_31.norm_1.weight\n",
            "Loaded variable: params.block_31.norm_2.weight\n",
            "Loaded variable: params.ln_f.weight\n",
            "Loaded variable: params.lm_head.kernel\n",
            "t=0 Hello, my name is\n",
            "t=1 Hello, my name is Ed\n",
            "t=2 Hello, my name is Ed.\n",
            "t=3 Hello, my name is Ed. Mr\n",
            "t=4 Hello, my name is Ed. Mr.\n",
            "t=5 Hello, my name is Ed. Mr. K\n",
            "t=6 Hello, my name is Ed. Mr. K has\n",
            "t=7 Hello, my name is Ed. Mr. K has passed\n",
            "t=8 Hello, my name is Ed. Mr. K has passed away\n",
            "t=9 Hello, my name is Ed. Mr. K has passed away.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kIcbxDMEgl3z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}