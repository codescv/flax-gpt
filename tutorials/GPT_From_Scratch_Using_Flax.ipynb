{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9182e6079fa74e85bc8236996ae71c27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b4dde54d0b254f748f37567add2c4c2e",
              "IPY_MODEL_fb7e233c430342d5ab389f41b3db3e28",
              "IPY_MODEL_6482de1485714e5c89bed2e3af3da849"
            ],
            "layout": "IPY_MODEL_bd9771c530ba4091ad3cca65531c6d02"
          }
        },
        "b4dde54d0b254f748f37567add2c4c2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3da42354b236476aaed7b0efde65feb8",
            "placeholder": "​",
            "style": "IPY_MODEL_4159c7314ca54900912641887270834c",
            "value": "train_loss=3.418: 100%"
          }
        },
        "fb7e233c430342d5ab389f41b3db3e28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0248efff3f94dd8b305ea4db85a0011",
            "max": 10000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_30d6db114ee0417aaf6943dacffcae08",
            "value": 10000
          }
        },
        "6482de1485714e5c89bed2e3af3da849": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b90867bbd5044bbd9d3bd08b66da288e",
            "placeholder": "​",
            "style": "IPY_MODEL_dd4d9236488e4b7fa2e2f6df19cf521d",
            "value": " 10000/10000 [24:33&lt;00:00,  1.21s/it]"
          }
        },
        "bd9771c530ba4091ad3cca65531c6d02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3da42354b236476aaed7b0efde65feb8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4159c7314ca54900912641887270834c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d0248efff3f94dd8b305ea4db85a0011": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30d6db114ee0417aaf6943dacffcae08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b90867bbd5044bbd9d3bd08b66da288e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd4d9236488e4b7fa2e2f6df19cf521d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Roadmap\n",
        "- Implement GPT from scratch using flax\n",
        "  - [x] Train a reference pytorch model (scaled down version of LLaMA2 \"tiny GPT\") using [Lit-GPT](https://github.com/Lightning-AI/lit-gpt)\n",
        "  - [x] Re-implement all layers using flax\n",
        "  - [x] The model should be numerically equivalent to the original model. We can verify this by loading a checkpoint from the pytorch model and compare the results.\n",
        "- [ ] do prediction on the tiny model\n",
        "- [ ] Load LLaMA-7B checkpoint and do prediction\n",
        "- [ ] train tiny GPT in jax to match the metrics of the reference model\n",
        "- [ ] Implement K-V cache in prediction\n",
        "- [ ] Finetuning\n",
        "- [ ] LoRA finetuning\n",
        "- [ ] Quantization\n",
        "- [ ] Distributed Training (TPUs)"
      ],
      "metadata": {
        "id": "xQ9HjCTWOaP6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "Ln7SC74FuqDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lit-GPT (for reference model)\n",
        "!pip install -U git+https://github.com/Lightning-AI/lit-gpt.git torchaudio torchdata torchtext torchvision"
      ],
      "metadata": {
        "id": "8502c7G4Qxgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Jax libraries\n",
        "!pip install einops git+https://github.com/google/CommonLoopUtils.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "immkG8wMurey",
        "outputId": "0292b837-d946-4c12-d879-e457d537cf37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/google/CommonLoopUtils.git\n",
            "  Cloning https://github.com/google/CommonLoopUtils.git to /tmp/pip-req-build-h6t7xoq0\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/google/CommonLoopUtils.git /tmp/pip-req-build-h6t7xoq0\n",
            "  Resolved https://github.com/google/CommonLoopUtils.git to commit 1368e52d0876dd0c90894793e8e9e97fc6f98adc\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting einops\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m762.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from clu==0.0.11) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.10/dist-packages (from clu==0.0.11) (1.6.0)\n",
            "Requirement already satisfied: flax in /usr/local/lib/python3.10/dist-packages (from clu==0.0.11) (0.8.1)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (from clu==0.0.11) (0.4.23)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (from clu==0.0.11) (0.4.23+cuda12.cudnn89)\n",
            "Collecting ml_collections (from clu==0.0.11)\n",
            "  Downloading ml_collections-0.1.1.tar.gz (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from clu==0.0.11) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from clu==0.0.11) (23.2)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from clu==0.0.11) (4.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from clu==0.0.11) (1.14.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath]->clu==0.0.11) (2023.6.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath]->clu==0.0.11) (6.1.1)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath]->clu==0.0.11) (3.17.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from flax->clu==0.0.11) (1.0.7)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.10/dist-packages (from flax->clu==0.0.11) (0.1.9)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.10/dist-packages (from flax->clu==0.0.11) (0.4.4)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.10/dist-packages (from flax->clu==0.0.11) (0.1.45)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.10/dist-packages (from flax->clu==0.0.11) (13.7.0)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from flax->clu==0.0.11) (6.0.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax->clu==0.0.11) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax->clu==0.0.11) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax->clu==0.0.11) (1.11.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from ml_collections->clu==0.0.11) (1.16.0)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.10/dist-packages (from ml_collections->clu==0.0.11) (21.6.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax->clu==0.0.11) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax->clu==0.0.11) (2.16.1)\n",
            "Requirement already satisfied: chex>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from optax->flax->clu==0.0.11) (0.1.7)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax->clu==0.0.11) (1.6.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax->clu==0.0.11) (3.20.3)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.7->optax->flax->clu==0.0.11) (0.1.8)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.7->optax->flax->clu==0.0.11) (0.12.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax->clu==0.0.11) (0.1.2)\n",
            "Building wheels for collected packages: clu, ml_collections\n",
            "  Building wheel for clu (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clu: filename=clu-0.0.11-py3-none-any.whl size=101429 sha256=c70b6248773b4de69f370649f2332a6e6df673e6b5e384e03d84e8d96cc3559f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-jg1ennwx/wheels/05/e0/0a/4f52d9eeba881fc02609adea7338fdf5181f5a7528663ebd83\n",
            "  Building wheel for ml_collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ml_collections: filename=ml_collections-0.1.1-py3-none-any.whl size=94505 sha256=e9be836225521a7b86ee01ae66e8990827f07048b60d6c5ffb48a1fe7a7f5189\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/89/c9/a9b87790789e94aadcfc393c283e3ecd5ab916aed0a31be8fe\n",
            "Successfully built clu ml_collections\n",
            "Installing collected packages: ml_collections, einops, clu\n",
            "Successfully installed clu-0.0.11 einops-0.7.0 ml_collections-0.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train a reference model\n",
        "\n",
        "Transformers are a bit more complicated than linear models. To ensure we have the correct implementation, we first train a scaled down version of the LLaMA2 model using Lit-GPT (we call it \"tiny GPT\"), and then use it as a reference.\n",
        "\n",
        "We are using the [Tiny shakespear dataset](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt) from Andrej Karpathy to train this model."
      ],
      "metadata": {
        "id": "lXA2dgUSN4u7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tiny GPT\n",
        "\n",
        "This tiny GPT model is a scaled down version of LLaMA2, with less layers, smaller head sizes and embedding sizes. This gives us a fast model for development.\n",
        "\n",
        "For convinence, I've pointed the checkpoint to Google Drive so I can reuse it without having to train it every time."
      ],
      "metadata": {
        "id": "hE113NpIT7df"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "checkpoint_root = '/content/drive/MyDrive/checkpoints'\n",
        "checkpoint_path = Path(checkpoint_root) / 'tiny_gpt'\n",
        "checkpoint_path.mkdir(exist_ok=True)"
      ],
      "metadata": {
        "id": "EKbXfdYtRd-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {checkpoint_path}/lit_config.json\n",
        "{\n",
        "  \"name\": \"tiny_gpt\",\n",
        "  \"block_size\": 128,\n",
        "  \"vocab_size\": 32000,\n",
        "  \"padding_multiple\": 64,\n",
        "  \"padded_vocab_size\": 32000,\n",
        "  \"n_layer\": 4,\n",
        "  \"n_head\": 4,\n",
        "  \"n_embd\": 128,\n",
        "  \"rotary_percentage\": 1.0,\n",
        "  \"parallel_residual\": false,\n",
        "  \"bias\": false,\n",
        "  \"lm_head_bias\": false,\n",
        "  \"n_query_groups\": 4,\n",
        "  \"shared_attention_norm\": false,\n",
        "  \"_norm_class\": \"RMSNorm\",\n",
        "  \"norm_eps\": 1e-05,\n",
        "  \"_mlp_class\": \"LLaMAMLP\",\n",
        "  \"gelu_approximate\": \"none\",\n",
        "  \"intermediate_size\": 512,\n",
        "  \"rope_condense_ratio\": 1,\n",
        "  \"rope_base\": 10000,\n",
        "  \"n_expert\": 0,\n",
        "  \"n_expert_per_token\": 0\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSZhrMInRJGm",
        "outputId": "07b8082e-7914-4c69-c7c1-50bad62c6c78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/MyDrive/checkpoints/tiny_gpt/lit_config.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also need the tokenizer file from LLaMA, which can be downloaded here:\n",
        "\n",
        "https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/tree/main\n",
        "\n",
        "download the `tokenizer.model` file and put it into the checkpoint dir."
      ],
      "metadata": {
        "id": "N-xQPjrxlh9y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Tiny GPT in pytorch"
      ],
      "metadata": {
        "id": "WrPQGIagt1WA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309,
          "referenced_widgets": [
            "9182e6079fa74e85bc8236996ae71c27",
            "b4dde54d0b254f748f37567add2c4c2e",
            "fb7e233c430342d5ab389f41b3db3e28",
            "6482de1485714e5c89bed2e3af3da849",
            "bd9771c530ba4091ad3cca65531c6d02",
            "3da42354b236476aaed7b0efde65feb8",
            "4159c7314ca54900912641887270834c",
            "d0248efff3f94dd8b305ea4db85a0011",
            "30d6db114ee0417aaf6943dacffcae08",
            "b90867bbd5044bbd9d3bd08b66da288e",
            "dd4d9236488e4b7fa2e2f6df19cf521d"
          ]
        },
        "id": "9FHWPzlgNzur",
        "outputId": "83b3b0b6-98e6-429f-ca43-f2847dee7f44"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9182e6079fa74e85bc8236996ae71c27"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step=0, validation loss=10.444548606872559\n",
            "step=2000, validation loss=4.689100742340088\n",
            "step=4000, validation loss=4.564824104309082\n",
            "step=6000, validation loss=4.602177143096924\n",
            "step=8000, validation loss=4.63491153717041\n",
            "step=9999, validation loss=4.567898750305176\n",
            "Test sampling model:\n",
            "Shakespear:\n",
            "My brother is the gates of seventeen,\n",
            "To wunder-a wage to cheek\n",
            "Who which he hath moved the court-trees her brother season on my lie\n",
            "you in them. Friar tune will teach his shame, my good heart!\n",
            "Antonio, mules extremes, and where poor man may be patiently\n",
            "ber-chender-twenty your highness uncartius,\n",
            "And stopsing eye in thy digressing rit the\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from tqdm.notebook import tqdm\n",
        "from lit_gpt import GPT, Config, Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(checkpoint_path)\n",
        "config = Config.from_json(checkpoint_path / \"lit_config.json\")\n",
        "\n",
        "if not os.path.exists('input.txt'):\n",
        "  !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "batch_size = 8\n",
        "block_size = 16\n",
        "\n",
        "data = tokenizer.encode(text).long()\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "\n",
        "def generate(model, tokenizer, prompt, max_tokens=100, temperature=1.0, context_window=50):\n",
        "  input = tokenizer.encode(prompt).view(1, -1)\n",
        "  eos_id=tokenizer.eos_id\n",
        "  model.eval()\n",
        "\n",
        "  result = [input[0]]\n",
        "\n",
        "  for _ in range(max_tokens):\n",
        "    # truncate\n",
        "    input = input[:, -context_window:]\n",
        "    with torch.no_grad():\n",
        "      logits = model(input)\n",
        "\n",
        "    next_token_logits = logits[0, -1, :]\n",
        "    probs = torch.nn.functional.softmax(next_token_logits / temperature, dim=-1)\n",
        "    next_token = torch.multinomial(probs, num_samples=1)\n",
        "    result.append(next_token)\n",
        "    if next_token.cpu().item() == eos_id:\n",
        "      break\n",
        "    input = torch.cat((input, next_token.view(1, -1)), dim=1)\n",
        "\n",
        "  return tokenizer.decode(torch.cat(result).cpu().numpy())\n",
        "\n",
        "\n",
        "model = GPT(config)\n",
        "\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "total_steps = 10000\n",
        "\n",
        "with tqdm(range(total_steps)) as pbar:\n",
        "  for step in pbar:\n",
        "    model.train()\n",
        "    xb, yb = get_batch('train')\n",
        "    B, T = xb.shape\n",
        "    logits = model(xb)\n",
        "    yb = yb.view(-1)\n",
        "    logits = logits.view(B*T, -1)\n",
        "    loss = F.cross_entropy(logits, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    pbar.set_description(f'train_loss={loss.item():.3f}')\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % (total_steps // 5) == 0 or step == total_steps-1:\n",
        "      model.eval()\n",
        "      losses = []\n",
        "      for _ in range(100):\n",
        "        with torch.no_grad():\n",
        "          xb, yb = get_batch('valid')\n",
        "          B, T = xb.shape\n",
        "          logits = model(xb)\n",
        "          yb = yb.view(-1)\n",
        "          logits = logits.view(B*T, -1)\n",
        "          loss = F.cross_entropy(logits, yb).item()\n",
        "          losses.append(loss)\n",
        "      avg_loss = torch.tensor(losses).mean().item()\n",
        "      print(f'{step=}, validation loss={avg_loss}')\n",
        "      # save checkpoint\n",
        "      model_ckpt_path = checkpoint_path / f'model-{step}.pth'\n",
        "      torch.save(model.state_dict(), model_ckpt_path)\n",
        "      shutil.copy(model_ckpt_path, checkpoint_path / 'lit_model.pth')\n",
        "\n",
        "print('Test sampling model:')\n",
        "print(generate(model, tokenizer, 'Shakespear:\\n', max_tokens=100, context_window=50))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5NmsTHqHnXJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT Components"
      ],
      "metadata": {
        "id": "oRfHlqQg4zTY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A LLaMA style GPT model (decoder-only transformer) looks like this:\n",
        "\n",
        "[![](https://mermaid.ink/img/pako:eNp9kc9rwyAUx_8VeScDbaA55rBD2WCHRsrWozBcfG0lUYMxjNL0f59R1w025uH9-H4_8sR3hdZKhBqOvf1oz8J5sj1wQ8JRZpg8Wa8fyJN-R0ljLJIX6-jNB9uhWYSZbHvbdptEvNgB6RKKhO3t-BtKNY2JbBYyoLGrslhlcS7LMt9l2WLF96S_ZlT_2-zHE1gEduKCjlmn6b3KI-594ppnFJLumrclfyFRjH5vT8qP3MAKNDotlAz_e10wDv6MGjnUoZTCdRy4uQVOTN6-XkwLtXcTrmAapPD4qMTJCQ31UfRjUFEqb12TFhb3dvsE9HGKjQ?type=png)](https://mermaid.live/edit#pako:eNp9kc9rwyAUx_8VeScDbaA55rBD2WCHRsrWozBcfG0lUYMxjNL0f59R1w025uH9-H4_8sR3hdZKhBqOvf1oz8J5sj1wQ8JRZpg8Wa8fyJN-R0ljLJIX6-jNB9uhWYSZbHvbdptEvNgB6RKKhO3t-BtKNY2JbBYyoLGrslhlcS7LMt9l2WLF96S_ZlT_2-zHE1gEduKCjlmn6b3KI-594ppnFJLumrclfyFRjH5vT8qP3MAKNDotlAz_e10wDv6MGjnUoZTCdRy4uQVOTN6-XkwLtXcTrmAapPD4qMTJCQ31UfRjUFEqb12TFhb3dvsE9HGKjQ)\n",
        "\n"
      ],
      "metadata": {
        "id": "2-uYCfl8zbh5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By inpsecting the reference model, we can know the layers in GPT model to be implemented: Embedding, Attention, MLP and LayerNorm(RMSNorm):"
      ],
      "metadata": {
        "id": "ZwRddKnMaAA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import lit_gpt\n",
        "from pathlib import Path\n",
        "import dataclasses\n",
        "import rich\n",
        "\n",
        "checkpoint_root = '/content/drive/MyDrive/checkpoints'\n",
        "checkpoint_path = Path(checkpoint_root) / 'tiny_gpt'\n",
        "\n",
        "lit_config = lit_gpt.Config.from_json(checkpoint_path / \"lit_config.json\")\n",
        "lit_model = lit_gpt.GPT(lit_config)\n",
        "rich.print('Model Config:', lit_config)\n",
        "rich.print('Model:', lit_model)"
      ],
      "metadata": {
        "id": "CegIBR_gzei3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 880
        },
        "outputId": "3a9f8ca9-d5f3-4a57-b362-610aae138673"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Model Config:\n",
              "\u001b[1;35mConfig\u001b[0m\u001b[1m(\u001b[0m\n",
              "    \u001b[33mname\u001b[0m=\u001b[32m'tiny_gpt'\u001b[0m,\n",
              "    \u001b[33mhf_config\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
              "    \u001b[33mblock_size\u001b[0m=\u001b[1;36m128\u001b[0m,\n",
              "    \u001b[33mvocab_size\u001b[0m=\u001b[1;36m32000\u001b[0m,\n",
              "    \u001b[33mpadding_multiple\u001b[0m=\u001b[1;36m64\u001b[0m,\n",
              "    \u001b[33mpadded_vocab_size\u001b[0m=\u001b[1;36m32000\u001b[0m,\n",
              "    \u001b[33mn_layer\u001b[0m=\u001b[1;36m4\u001b[0m,\n",
              "    \u001b[33mn_head\u001b[0m=\u001b[1;36m4\u001b[0m,\n",
              "    \u001b[33mn_embd\u001b[0m=\u001b[1;36m128\u001b[0m,\n",
              "    \u001b[33mrotary_percentage\u001b[0m=\u001b[1;36m1\u001b[0m\u001b[1;36m.0\u001b[0m,\n",
              "    \u001b[33mparallel_residual\u001b[0m=\u001b[3;91mFalse\u001b[0m,\n",
              "    \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m,\n",
              "    \u001b[33mlm_head_bias\u001b[0m=\u001b[3;91mFalse\u001b[0m,\n",
              "    \u001b[33mn_query_groups\u001b[0m=\u001b[1;36m4\u001b[0m,\n",
              "    \u001b[33mshared_attention_norm\u001b[0m=\u001b[3;91mFalse\u001b[0m,\n",
              "    \u001b[33m_norm_class\u001b[0m=\u001b[32m'RMSNorm'\u001b[0m,\n",
              "    \u001b[33mnorm_eps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m,\n",
              "    \u001b[33m_mlp_class\u001b[0m=\u001b[32m'LLaMAMLP'\u001b[0m,\n",
              "    \u001b[33mgelu_approximate\u001b[0m=\u001b[32m'none'\u001b[0m,\n",
              "    \u001b[33mintermediate_size\u001b[0m=\u001b[1;36m512\u001b[0m,\n",
              "    \u001b[33mrope_condense_ratio\u001b[0m=\u001b[1;36m1\u001b[0m,\n",
              "    \u001b[33mrope_base\u001b[0m=\u001b[1;36m10000\u001b[0m,\n",
              "    \u001b[33mn_expert\u001b[0m=\u001b[1;36m0\u001b[0m,\n",
              "    \u001b[33mn_expert_per_token\u001b[0m=\u001b[1;36m0\u001b[0m\n",
              "\u001b[1m)\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Model Config:\n",
              "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Config</span><span style=\"font-weight: bold\">(</span>\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'tiny_gpt'</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">hf_config</span>=<span style=\"font-weight: bold\">{}</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">block_size</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">vocab_size</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32000</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">padding_multiple</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">padded_vocab_size</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32000</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">n_layer</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">n_head</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">n_embd</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">rotary_percentage</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">parallel_residual</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">lm_head_bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">n_query_groups</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">shared_attention_norm</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">_norm_class</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'RMSNorm'</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">norm_eps</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1e-05</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">_mlp_class</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'LLaMAMLP'</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">gelu_approximate</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'none'</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">intermediate_size</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">rope_condense_ratio</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">rope_base</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10000</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">n_expert</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">n_expert_per_token</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
              "<span style=\"font-weight: bold\">)</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Model: \u001b[1;35mGPT\u001b[0m\u001b[1m(\u001b[0m\n",
              "  \u001b[1m(\u001b[0mlm_head\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m128\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m32000\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
              "  \u001b[1m(\u001b[0mtransformer\u001b[1m)\u001b[0m: \u001b[1;35mModuleDict\u001b[0m\u001b[1m(\u001b[0m\n",
              "    \u001b[1m(\u001b[0mwte\u001b[1m)\u001b[0m: \u001b[1;35mEmbedding\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m32000\u001b[0m, \u001b[1;36m128\u001b[0m\u001b[1m)\u001b[0m\n",
              "    \u001b[1m(\u001b[0mh\u001b[1m)\u001b[0m: \u001b[1;35mModuleList\u001b[0m\u001b[1m(\u001b[0m\n",
              "      \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m-\u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m4\u001b[0m x \u001b[1;35mBlock\u001b[0m\u001b[1m(\u001b[0m\n",
              "        \u001b[1m(\u001b[0mnorm_1\u001b[1m)\u001b[0m: \u001b[1;35mRMSNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
              "        \u001b[1m(\u001b[0mattn\u001b[1m)\u001b[0m: \u001b[1;35mCausalSelfAttention\u001b[0m\u001b[1m(\u001b[0m\n",
              "          \u001b[1m(\u001b[0mattn\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m128\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
              "          \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m128\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m128\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
              "        \u001b[1m)\u001b[0m\n",
              "        \u001b[1m(\u001b[0mnorm_2\u001b[1m)\u001b[0m: \u001b[1;35mRMSNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
              "        \u001b[1m(\u001b[0mmlp\u001b[1m)\u001b[0m: \u001b[1;35mLLaMAMLP\u001b[0m\u001b[1m(\u001b[0m\n",
              "          \u001b[1m(\u001b[0mfc_1\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m128\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m512\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
              "          \u001b[1m(\u001b[0mfc_2\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m128\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m512\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
              "          \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m512\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m128\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
              "        \u001b[1m)\u001b[0m\n",
              "      \u001b[1m)\u001b[0m\n",
              "    \u001b[1m)\u001b[0m\n",
              "    \u001b[1m(\u001b[0mln_f\u001b[1m)\u001b[0m: \u001b[1;35mRMSNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
              "  \u001b[1m)\u001b[0m\n",
              "\u001b[1m)\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Model: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">GPT</span><span style=\"font-weight: bold\">(</span>\n",
              "  <span style=\"font-weight: bold\">(</span>lm_head<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32000</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n",
              "  <span style=\"font-weight: bold\">(</span>transformer<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ModuleDict</span><span style=\"font-weight: bold\">(</span>\n",
              "    <span style=\"font-weight: bold\">(</span>wte<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Embedding</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32000</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span><span style=\"font-weight: bold\">)</span>\n",
              "    <span style=\"font-weight: bold\">(</span>h<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ModuleList</span><span style=\"font-weight: bold\">(</span>\n",
              "      <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> x <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Block</span><span style=\"font-weight: bold\">(</span>\n",
              "        <span style=\"font-weight: bold\">(</span>norm_1<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RMSNorm</span><span style=\"font-weight: bold\">()</span>\n",
              "        <span style=\"font-weight: bold\">(</span>attn<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">CausalSelfAttention</span><span style=\"font-weight: bold\">(</span>\n",
              "          <span style=\"font-weight: bold\">(</span>attn<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">384</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n",
              "          <span style=\"font-weight: bold\">(</span>proj<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n",
              "        <span style=\"font-weight: bold\">)</span>\n",
              "        <span style=\"font-weight: bold\">(</span>norm_2<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RMSNorm</span><span style=\"font-weight: bold\">()</span>\n",
              "        <span style=\"font-weight: bold\">(</span>mlp<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LLaMAMLP</span><span style=\"font-weight: bold\">(</span>\n",
              "          <span style=\"font-weight: bold\">(</span>fc_1<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n",
              "          <span style=\"font-weight: bold\">(</span>fc_2<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n",
              "          <span style=\"font-weight: bold\">(</span>proj<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n",
              "        <span style=\"font-weight: bold\">)</span>\n",
              "      <span style=\"font-weight: bold\">)</span>\n",
              "    <span style=\"font-weight: bold\">)</span>\n",
              "    <span style=\"font-weight: bold\">(</span>ln_f<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RMSNorm</span><span style=\"font-weight: bold\">()</span>\n",
              "  <span style=\"font-weight: bold\">)</span>\n",
              "<span style=\"font-weight: bold\">)</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config\n",
        "\n",
        "To avoid passing paramters from parent layers to sublayers, we define a config (like in Lit-GPT) for the whole transformer and pass it to every sub layer:"
      ],
      "metadata": {
        "id": "rmMDiK_bdZp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dataclasses\n",
        "import torch\n",
        "from flax import linen as nn\n",
        "import jax\n",
        "import jax.numpy as jnp, jax.random as jrandom\n",
        "import einops\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class Config:\n",
        "  max_seq_length: int      # maximum context length\n",
        "  vocab_size: int          # vocabulary size\n",
        "  n_embed: int             # embedding size (= n_head * head_size)\n",
        "  n_layer: int             # number of transformer blocks\n",
        "  intermediate_size: int   # intermediate size of FFN\n",
        "\n",
        "  # multi head / multi query attention\n",
        "  n_head: int              # number of heads\n",
        "  n_query_groups: int      # number of query groups in multi-query attention\n",
        "\n",
        "  # RoPE positional embedding\n",
        "  rope_condense_ratio: int # rope condense ratio\n",
        "  rope_base: int           # rope base\n",
        "\n",
        "  def __post_init__(self):\n",
        "    assert self.n_embed % self.n_head == 0, f'Embedding size(n_embed={self.n_embed}) should be divisible by (n_head={self.n_head})'\n",
        "    self.head_size = self.n_embed // self.n_head\n"
      ],
      "metadata": {
        "id": "LPjB_dr0daNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = Config(\n",
        "    vocab_size=lit_config.vocab_size,\n",
        "    n_embed=lit_config.n_embd,\n",
        "    n_head=lit_config.n_head,\n",
        "    n_layer=lit_config.n_layer,\n",
        "    intermediate_size=lit_config.intermediate_size,\n",
        "    n_query_groups=lit_config.n_query_groups,\n",
        "    max_seq_length=lit_config.block_size,\n",
        "    rope_condense_ratio=lit_config.rope_condense_ratio,\n",
        "    rope_base=lit_config.rope_base\n",
        ")\n",
        "\n",
        "rich.print('Model Config:', config)"
      ],
      "metadata": {
        "id": "0tGuPsujuX6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Block\n",
        "\n",
        "A transformer block looks like this: (tip: you can use https://mermaid.live/ to create diagrams like this)\n",
        "\n",
        "[![](https://mermaid.ink/img/pako:eNplj70OgjAQgF-F3CQRBhgZTDSOYgw4dmlolUbaknqNMcC7W2lA1E5333d_7aDSjEMGl0Y_qpoaDHZnogL3hGotBnG8CYq8PGojE8-nbFRbRK5QaOXdnPq-5HdSX_C7YJY2_SyLZLVah-FyUfq1KB1Vfjh56gJfm_73L8dPOv1obdEd4jlEILmRVDD3-e7NCGDNJSeQuZBRcyNA1ODqqEVdPlUFGRrLI7Ato8j3gl4NlR4OLxn1YEI?type=png)](https://mermaid.live/edit#pako:eNplj70OgjAQgF-F3CQRBhgZTDSOYgw4dmlolUbaknqNMcC7W2lA1E5333d_7aDSjEMGl0Y_qpoaDHZnogL3hGotBnG8CYq8PGojE8-nbFRbRK5QaOXdnPq-5HdSX_C7YJY2_SyLZLVah-FyUfq1KB1Vfjh56gJfm_73L8dPOv1obdEd4jlEILmRVDD3-e7NCGDNJSeQuZBRcyNA1ODqqEVdPlUFGRrLI7Ato8j3gl4NlR4OLxn1YEI)"
      ],
      "metadata": {
        "id": "eZY4ifr9XnLu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To implement a Block, we need to implement RMSNorm and Attention."
      ],
      "metadata": {
        "id": "6q49YBx6sDLT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RMSNorm\n",
        "The original layernorm normalizes to a standard normal distribution:\n",
        "$\\frac{x - \\bar{x}}{ \\sqrt{\\sigma^2 + \\epsilon}}$\n",
        "\n",
        "The RMS layernorm normalizes to a vector of norm $\\sqrt{N}$:\n",
        "$\\frac{x}{\\sqrt{\\frac{|x|^2}{N} + \\epsilon}}$\n",
        "\n",
        "Where N is the layer dimension."
      ],
      "metadata": {
        "id": "LXPYfwIDsNC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RMSNorm(nn.Module):\n",
        "  \"\"\"Root Mean Square Layer Normalization.\n",
        "\n",
        "  Derived from https://github.com/bzhangGo/rmsnorm/blob/master/rmsnorm_torch.py. BSD 3-Clause License:\n",
        "  https://github.com/bzhangGo/rmsnorm/blob/master/LICENSE.\n",
        "  \"\"\"\n",
        "  axis: int = -1\n",
        "  eps: float = 1e-5\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    weight = self.param('weight', lambda rng, shape: jnp.ones(shape), x.shape[-1])\n",
        "    norm_x = jnp.mean(x * x, axis=self.axis, keepdims=True)\n",
        "    x_normed = x / jnp.sqrt(norm_x + self.eps)\n",
        "    return weight * x_normed"
      ],
      "metadata": {
        "id": "BiJUXSfSXdST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tests"
      ],
      "metadata": {
        "id": "KkjzIhGgkzWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator = torch.Generator().manual_seed(1337)\n",
        "rmsnorm_lit = lit_gpt.rmsnorm.RMSNorm(size=lit_config.n_embd)\n",
        "rmsnorm = RMSNorm()\n",
        "# input\n",
        "B, T, C = 1, 5, lit_config.n_embd\n",
        "x = torch.randn((B, T, C), generator=generator)\n",
        "# run the reference RMSNorm\n",
        "with torch.no_grad():\n",
        "  out1 = rmsnorm_lit(x)\n",
        "\n",
        "variables = rmsnorm.init(jrandom.key(0), x=x.numpy())\n",
        "# copy state from reference RMSNorm\n",
        "variables['params']['weight'] = rmsnorm_lit.state_dict()['weight'].numpy()\n",
        "\n",
        "out2 = rmsnorm.apply(variables, x.numpy())\n",
        "\n",
        "assert jnp.allclose(out1.numpy(), out2)\n",
        "print('Normalized norm should be close to 1:', jnp.linalg.norm(out2[0,0] / variables['params']['weight'] / jnp.sqrt(x.shape[-1])))\n",
        "print('Test passed: RMS')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jiVhL2AkyyC",
        "outputId": "24bb2113-1559-4b58-c6c8-3e4dcde160be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized norm should be close to 1: 0.99999535\n",
            "Test passed: RMS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rotary Positional Embedding (RoPE)"
      ],
      "metadata": {
        "id": "M_SxKPOiqrjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_rope_cache(\n",
        "    seq_len: int,\n",
        "    n_elem: int,\n",
        "    base: int = 10000,\n",
        "    condense_ratio: int = 1\n",
        "):\n",
        "  \"\"\"Enhanced Transformer with Rotary Position Embedding.\n",
        "\n",
        "  Derived from: https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/\n",
        "  transformers/rope/__init__.py. MIT License:\n",
        "  https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/license.\n",
        "  \"\"\"\n",
        "  # $\\Theta = {\\theta_i = 10000^{\\frac{2(i-1)}{d}}, i \\in [1, 2, ..., \\frac{d}{2}]}$\n",
        "  theta = 1.0 / (base ** (jnp.arange(0, n_elem, 2) / n_elem))\n",
        "\n",
        "  # Create position indexes `[0, 1, ..., seq_len - 1]`\n",
        "  seq_idx = jnp.arange(seq_len) / condense_ratio\n",
        "\n",
        "  # Calculate the product of position index and $\\theta_i$\n",
        "  idx_theta = jnp.outer(seq_idx, theta)\n",
        "  idx_theta = jnp.tile(idx_theta, (1, 2))\n",
        "\n",
        "  return jnp.stack([jnp.cos(idx_theta), jnp.sin(idx_theta)], axis=-1)\n",
        "\n",
        "\n",
        "def apply_rope(x, rope_emb):\n",
        "  cos, sin = rope_emb[..., 0], rope_emb[..., 1]\n",
        "  head_size = x.shape[-1]\n",
        "  x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)\n",
        "  x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)\n",
        "  rotated = jnp.concatenate((-x2, x1), axis=-1)  # (B, nh, T, hs)\n",
        "  roped = (x * cos) + (rotated * sin)\n",
        "  return roped"
      ],
      "metadata": {
        "id": "wag5yoWfqyqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tests"
      ],
      "metadata": {
        "id": "vdfWkzpM4abb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator = torch.Generator().manual_seed(1337)\n",
        "input_shape = (B, n_heads, T, head_size) = (1, 4, 10, 32)\n",
        "x = torch.randn(input_shape, generator=generator)\n",
        "cos, sin = lit_gpt.model.build_rope_cache(seq_len=128, n_elem=head_size, base=10000, condense_ratio=1)\n",
        "expected_output = lit_gpt.model.apply_rope(x, cos[:T], sin[:T])\n",
        "\n",
        "rope_emb = build_rope_cache(seq_len=128, n_elem=head_size, base=10000, condense_ratio=1)\n",
        "y = apply_rope(x.numpy(), rope_emb[:T])\n",
        "assert jnp.allclose(y, expected_output.numpy(), rtol=1e-5), \"rope embedding doesn't match\"\n",
        "print('Test passed: RoPE')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZs6NV-TrUqH",
        "outputId": "48aae29f-5632-4765-f142-a566395c5219"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test passed: RoPE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self Attention"
      ],
      "metadata": {
        "id": "1jSJezz2sO4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "  \"\"\"Multi head / Multi query / Grouped Query Attention.\n",
        "\n",
        "  About n_query_groups\n",
        "  to use multi-head attention (MHA), set this to `n_head` (default)\n",
        "  to use multi-query attention (MQA), set this to 1\n",
        "  to use grouped-query attention (GQA), set this to a value in between\n",
        "  Example with `n_head=4`\n",
        "  ┌───┐┌───┐┌───┐┌───┐     ┌───┐    ┌───┐             ┌───┐\n",
        "  │ v ││ v ││ v ││ v │     │ v │    │ v │             │ v │\n",
        "  └───┘└───┘└───┘└───┘     └───┘    └───┘             └───┘\n",
        "    │    │    │    │         │        │                 │\n",
        "  ┌───┐┌───┐┌───┐┌───┐     ┌───┐    ┌───┐             ┌───┐\n",
        "  │ k ││ k ││ k ││ k │     │ k │    │ k │             │ k │\n",
        "  └───┘└───┘└───┘└───┘     └───┘    └───┘             └───┘\n",
        "    │    │    │    │      ┌──┴──┐  ┌──┴──┐      ┌────┬──┴─┬────┐\n",
        "  ┌───┐┌───┐┌───┐┌───┐  ┌───┐┌───┐┌───┐┌───┐  ┌───┐┌───┐┌───┐┌───┐\n",
        "  │ q ││ q ││ q ││ q │  │ q ││ q ││ q ││ q │  │ q ││ q ││ q ││ q │\n",
        "  └───┘└───┘└───┘└───┘  └───┘└───┘└───┘└───┘  └───┘└───┘└───┘└───┘\n",
        "  ◀──────────────────▶  ◀──────────────────▶  ◀──────────────────▶\n",
        "          MHA                    GQA                   MQA\n",
        "    n_query_groups=4       n_query_groups=2      n_query_groups=1\n",
        "    q_per_kv=1             q_per_kv=2            q_per_kv=4\n",
        "    n_head=4               n_head=4              n_head=4\n",
        "    n_qkv=3                n_qkv=4               n_qkv=6\n",
        "  credit https://arxiv.org/pdf/2305.13245.pdf\n",
        "  \"\"\"\n",
        "  config: Config\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x, rope_emb, mask=None):\n",
        "    T = x.shape[-2]  # x: (B, T, C)\n",
        "    mask = mask or jnp.tril(jnp.ones((T, T)))\n",
        "\n",
        "    # nq = n_head, nk = nv = n_query_groups\n",
        "    qkv_dim = (self.config.n_head + 2 * self.config.n_query_groups) * self.config.head_size\n",
        "    qkv_proj = nn.Dense(features=qkv_dim, use_bias=False, name='proj_qkv')(x)\n",
        "\n",
        "    # number of q's per group\n",
        "    q_per_kv = self.config.n_head // self.config.n_query_groups\n",
        "    # number of qkvs per group, k=v=1\n",
        "    n_qkv = q_per_kv + 2\n",
        "    # break embedding into (n_groups, n_qkv, head_size)\n",
        "    qkv = einops.rearrange(qkv_proj, 'b t (n_groups n_qkv h) -> b n_groups n_qkv t h',\n",
        "                           n_groups=self.config.n_query_groups,\n",
        "                           n_qkv=n_qkv)\n",
        "    # split q, k, v within groups\n",
        "    q, k, v = einops.unpack(qkv, [[q_per_kv], [1], [1]], 'b n_groups * t h')\n",
        "\n",
        "    if q_per_kv != 1:\n",
        "      # repeat k and v in each group\n",
        "      k = einops.repeat(k, 'b n_groups 1 t h -> b n_groups q_per_kv t h', q_per_kv=q_per_kv)\n",
        "      v = einops.repeat(v, 'b n_groups 1 t h -> b n_groups q_per_kv t h', q_per_kv=q_per_kv)\n",
        "\n",
        "    # merge groups into heads\n",
        "    q = einops.rearrange(q, 'b n_groups q_per_kv t h -> b (n_groups q_per_kv) t h')\n",
        "    k = einops.rearrange(k, 'b n_groups q_per_kv t h -> b (n_groups q_per_kv) t h')\n",
        "    v = einops.rearrange(v, 'b n_groups q_per_kv t h -> b (n_groups q_per_kv) t h')\n",
        "\n",
        "    # apply position embedding\n",
        "    # NOTE: only apply to q and k, but not v\n",
        "    q = apply_rope(q, rope_emb)\n",
        "    k = apply_rope(k, rope_emb)\n",
        "\n",
        "    # multi head scaled dot attention\n",
        "    weights = einops.einsum(q, k, 'b nh tq h, b nh tk h -> b nh tq tk')\n",
        "    weights = weights / jnp.sqrt(self.config.head_size)\n",
        "    weights = jnp.where(mask, weights, float('-inf'))\n",
        "    weights = nn.softmax(weights, axis=-1)\n",
        "    out = einops.einsum(weights, v, 'b nh tq tv, b nh tv h -> b tq nh h')\n",
        "\n",
        "    # concat heads\n",
        "    out = einops.rearrange(out, 'b t nh h -> b t (nh h)')\n",
        "\n",
        "    # final projection\n",
        "    out = nn.Dense(self.config.n_embed, use_bias=False, name='proj_out')(out)\n",
        "\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "iVU8TzcytUAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tests"
      ],
      "metadata": {
        "id": "8N5RWPaYxLZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "generator = torch.Generator().manual_seed(1337)\n",
        "B, T, C = 1, 5, lit_config.n_embd\n",
        "x = torch.randn(B, T, C, generator=generator)\n",
        "attn_lit = lit_gpt.model.CausalSelfAttention(lit_config)\n",
        "with torch.no_grad():\n",
        "  cos, sin = lit_gpt.model.build_rope_cache(seq_len=lit_config.block_size, n_elem=lit_config.head_size, base=10000, condense_ratio=1)\n",
        "  out1 = attn_lit(x, cos[:T], sin[:T])\n",
        "\n",
        "attn = SelfAttention(config)\n",
        "rope_emb = build_rope_cache(seq_len=config.max_seq_length, n_elem=config.head_size, base=10000, condense_ratio=1)\n",
        "variables = attn.init(jrandom.key(0), x.numpy(), rope_emb[:T])\n",
        "# copy weights\n",
        "variables['params']['proj_qkv']['kernel'] = attn_lit.state_dict()['attn.weight'].T.numpy()\n",
        "variables['params']['proj_out']['kernel'] = attn_lit.state_dict()['proj.weight'].T.numpy()\n",
        "out2 = attn.apply(variables, x.numpy(), rope_emb[:T], mask=None)\n",
        "\n",
        "assert jnp.allclose(out1.numpy(), out2, rtol=1e-4)\n",
        "print('Test passed: SelfAttention')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rw2eQ0YtxKzD",
        "outputId": "595b97e7-91b9-47b0-c97a-f288b562cb6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test passed: SelfAttention\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP"
      ],
      "metadata": {
        "id": "fXEqI48xdyuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  \"\"\"LLaMA style MLP.\"\"\"\n",
        "  config: Config\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    x1 = nn.Dense(self.config.intermediate_size, use_bias=False, name='fc_1')(x)\n",
        "    x2 = nn.Dense(self.config.intermediate_size, use_bias=False, name='fc_2')(x)\n",
        "    x = nn.silu(x1) * x2\n",
        "    x = nn.Dense(self.config.n_embed, use_bias=False, name='proj_out')(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "qrcXFo2yd1uS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tests"
      ],
      "metadata": {
        "id": "O0uy9E19d2mE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "generator = torch.Generator().manual_seed(1337)\n",
        "B, T, C = 1, 5, lit_config.n_embd\n",
        "x = torch.randn(B, T, C, generator=generator)\n",
        "mlp_lit = lit_gpt.model.LLaMAMLP(lit_config)\n",
        "with torch.no_grad():\n",
        "  out1 = mlp_lit(x)\n",
        "\n",
        "mlp = MLP(config)\n",
        "variables = mlp.init(jrandom.key(0), x.numpy())\n",
        "variables['params']['fc_1']['kernel'] = mlp_lit.state_dict()['fc_1.weight'].T.numpy()\n",
        "variables['params']['fc_2']['kernel'] = mlp_lit.state_dict()['fc_2.weight'].T.numpy()\n",
        "variables['params']['proj_out']['kernel'] = mlp_lit.state_dict()['proj.weight'].T.numpy()\n",
        "\n",
        "out2 = mlp.apply(variables, x.numpy())\n",
        "assert jnp.allclose(out1.numpy(), out2, rtol=1e-4)\n",
        "print('Test passed: MLP', out1.shape, out2.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0prRD3Dtd6H1",
        "outputId": "45543ed6-1cf3-4013-ea0e-4fde577032ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test passed: MLP torch.Size([1, 5, 128]) (1, 5, 128)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Block\n"
      ],
      "metadata": {
        "id": "O1MBPVVucuHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "  config: Config\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x, rope_emb, mask=None):\n",
        "    n1 = RMSNorm(name='norm_1')(x)\n",
        "    h = SelfAttention(self.config, name='attn')(n1, rope_emb, mask=mask)\n",
        "    x = h + x\n",
        "    n2 = RMSNorm(name='norm_2')(x)\n",
        "    h = MLP(self.config, name='mlp')(n2)\n",
        "    x = h + x\n",
        "    return x"
      ],
      "metadata": {
        "id": "KcwEitMGdAEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tests"
      ],
      "metadata": {
        "id": "FLPs-hE3dAZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "generator = torch.Generator().manual_seed(1337)\n",
        "B, T, C = 1, 5, lit_config.n_embd\n",
        "x = torch.randn(B, T, C, generator=generator)\n",
        "block_lit = lit_gpt.model.Block(lit_config)\n",
        "with torch.no_grad():\n",
        "  cos, sin = lit_gpt.model.build_rope_cache(seq_len=lit_config.block_size, n_elem=lit_config.head_size, base=10000, condense_ratio=1)\n",
        "  out1 = block_lit(x, cos[:T], sin[:T])\n",
        "\n",
        "block = Block(config)\n",
        "rope_emb = build_rope_cache(seq_len=config.max_seq_length, n_elem=config.head_size, base=10000, condense_ratio=1)\n",
        "variables = block.init(jrandom.key(0), x.numpy(), rope_emb[:T])\n",
        "variables['params']['norm_1']['weight'] = block_lit.state_dict()['norm_1.weight'].numpy()\n",
        "variables['params']['norm_2']['weight'] = block_lit.state_dict()['norm_2.weight'].numpy()\n",
        "variables['params']['attn']['proj_qkv']['kernel'] = block_lit.state_dict()['attn.attn.weight'].T.numpy()\n",
        "variables['params']['attn']['proj_out']['kernel'] = block_lit.state_dict()['attn.proj.weight'].T.numpy()\n",
        "variables['params']['mlp']['fc_1']['kernel'] = block_lit.state_dict()['mlp.fc_1.weight'].T.numpy()\n",
        "variables['params']['mlp']['fc_2']['kernel'] = block_lit.state_dict()['mlp.fc_2.weight'].T.numpy()\n",
        "variables['params']['mlp']['proj_out']['kernel'] = block_lit.state_dict()['mlp.proj.weight'].T.numpy()\n",
        "\n",
        "\n",
        "out2 = block.apply(variables, x.numpy(), rope_emb[:T])\n",
        "\n",
        "print(out1.shape, out2.shape)\n",
        "assert jnp.allclose(out1.numpy(), out2, rtol=1e-4)\n",
        "print('Test passed: Block')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tvf4qB1uc-PH",
        "outputId": "c8e62680-fe4c-4d2a-9248-f1e94bf04405"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 5, 128]) (1, 5, 128)\n",
            "Test passed: Block\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer"
      ],
      "metadata": {
        "id": "RBSDxCxzXOhQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "  config: Config\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    T = x.shape[-1]  # (B, T)\n",
        "    rope_emb = self.variable('cache', 'rope_emb', build_rope_cache,\n",
        "                             self.config.max_seq_length,\n",
        "                             self.config.head_size,\n",
        "                             self.config.rope_base,\n",
        "                             self.config.rope_condense_ratio)\n",
        "\n",
        "    x = nn.Embed(num_embeddings=self.config.vocab_size,\n",
        "                 features=self.config.n_embed, name='emb')(x)\n",
        "    self.sow('intermediates', 'emb_out', x)\n",
        "\n",
        "    for i in range(self.config.n_layer):\n",
        "      x = Block(config=self.config, name=f'block_{i}')(x, rope_emb.value[:T])\n",
        "      self.sow('intermediates', f'block_{i}_out', x)\n",
        "\n",
        "    # final layer norm\n",
        "    x = RMSNorm(name='ln_f')(x)\n",
        "    self.sow('intermediates', 'ln_out', x)\n",
        "    # language model head\n",
        "    x = nn.Dense(self.config.vocab_size, name='lm_head', use_bias=False)(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "nIGpAheA5GdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tests\n",
        "\n",
        "We first get all intermediate outputs from reference model, then compare them one by one with our implementation."
      ],
      "metadata": {
        "id": "4j2pDmxd5W_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# our test input\n",
        "tokenizer = lit_gpt.Tokenizer(checkpoint_path)\n",
        "idx = tokenizer.encode('Hello, my name is').view(1, -1)\n",
        "\n",
        "lit_model.load_state_dict(torch.load(checkpoint_path / 'lit_model.pth'))\n",
        "\n",
        "with torch.no_grad():\n",
        "  # Calling model end-to-end\n",
        "  out1 = lit_model(idx)\n",
        "\n",
        "  # Calling model layer by layer\n",
        "  # token embedding\n",
        "  token_emb = lit_model.transformer.wte(idx)\n",
        "  T = idx.size(1)\n",
        "\n",
        "  # rope embeddings\n",
        "  cos, sin = lit_model.cos[:T], lit_model.sin[:T]\n",
        "\n",
        "  # transformer blocks\n",
        "  hidden_results = []\n",
        "  h = token_emb\n",
        "  for block in lit_model.transformer.h:\n",
        "      h_out = block(h, cos, sin, mask=None, input_pos=None)\n",
        "      hidden_results.append({'input': h.numpy(), 'output': h_out.numpy()})\n",
        "      h = h_out\n",
        "\n",
        "  # final layer norm\n",
        "  ln_result = lit_model.transformer.ln_f(h)\n",
        "  # transformer output\n",
        "  out2 = lit_model.lm_head(ln_result)\n",
        "\n",
        "  # store all expected results\n",
        "  expected_intermediates = {\n",
        "      'emb': {'input': idx.numpy(), 'output': token_emb.numpy()},\n",
        "      'blocks': hidden_results,\n",
        "      'ln': {'input': hidden_results[-1]['output'], 'output': ln_result.numpy()},\n",
        "      'lm_head': {'input': ln_result.numpy(), 'output': out2.numpy()},\n",
        "      'rope': {'sin': lit_model.sin.numpy(), 'cos': lit_model.cos.numpy()},\n",
        "  }\n",
        "\n",
        "assert torch.allclose(out1, out2), \"model output doesn't match\"\n",
        "\n",
        "model = GPT(config)\n",
        "# variables = model.init(jrandom.key(0), x=idx.numpy())\n",
        "variables = model.lazy_init(jrandom.key(0), x=idx.numpy())\n",
        "\n",
        "state_dict = lit_model.state_dict().copy()\n",
        "variables['params']['lm_head']['kernel'] = state_dict.pop('lm_head.weight').T.numpy()\n",
        "variables['params']['emb']['embedding'] = state_dict.pop('transformer.wte.weight').numpy()\n",
        "variables['params']['ln_f']['weight'] = state_dict.pop('transformer.ln_f.weight').numpy()\n",
        "\n",
        "for i in range(config.n_layer):\n",
        "  variables['params'][f'block_{i}']['norm_1']['weight'] = state_dict.pop(f'transformer.h.{i}.norm_1.weight').numpy()\n",
        "  variables['params'][f'block_{i}']['norm_2']['weight'] = state_dict.pop(f'transformer.h.{i}.norm_2.weight').numpy()\n",
        "  variables['params'][f'block_{i}']['attn']['proj_qkv']['kernel'] = state_dict.pop(f'transformer.h.{i}.attn.attn.weight').T.numpy()\n",
        "  variables['params'][f'block_{i}']['attn']['proj_out']['kernel'] = state_dict.pop(f'transformer.h.{i}.attn.proj.weight').T.numpy()\n",
        "  variables['params'][f'block_{i}']['mlp']['fc_1']['kernel'] = state_dict.pop(f'transformer.h.{i}.mlp.fc_1.weight').T.numpy()\n",
        "  variables['params'][f'block_{i}']['mlp']['fc_2']['kernel'] = state_dict.pop(f'transformer.h.{i}.mlp.fc_2.weight').T.numpy()\n",
        "  variables['params'][f'block_{i}']['mlp']['proj_out']['kernel'] = state_dict.pop(f'transformer.h.{i}.mlp.proj.weight').T.numpy()\n",
        "\n",
        "assert len(state_dict.keys()) == 0, f'State not loaded: {state_dict.keys()}'\n",
        "\n",
        "out3, states = model.apply(variables, x=idx.numpy(), mutable=['intermediates'])\n",
        "intermediates = states['intermediates']\n",
        "\n",
        "# embedding\n",
        "assert jnp.allclose(expected_intermediates['emb']['output'], intermediates['emb_out'][0]), 'Emb not match'\n",
        "\n",
        "# blocks\n",
        "for i in range(config.n_layer):\n",
        "  assert jnp.allclose(expected_intermediates['blocks'][i]['output'], intermediates[f'block_{i}_out'][0], rtol=1e-4, atol=1e-5), f'Block {i} not match'\n",
        "\n",
        "# final ln\n",
        "assert jnp.allclose(expected_intermediates['ln']['output'], intermediates['ln_out'][0], rtol=1e-4, atol=1e-6), 'final layer norm not match'\n",
        "\n",
        "# End to end result\n",
        "assert jnp.allclose(out1.numpy(), out3, rtol=1e-4, atol=1e-5)\n",
        "\n",
        "print('All tests passed: Transformer')"
      ],
      "metadata": {
        "id": "aVr9QgckWWsw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e70bfa8c-5739-49a6-8eb7-080d51615ea6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests passed: Transformer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generation"
      ],
      "metadata": {
        "id": "mqZQeAh_HhZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(key, model, tokenizer, variables, prompt, max_tokens=100):\n",
        "  x = tokenizer.encode(prompt).view(1, -1).numpy()\n",
        "  result = x[0].tolist()\n",
        "\n",
        "  for t in range(max_tokens):\n",
        "    x = x[..., -model.config.max_seq_length:]\n",
        "    logits = model.apply(variables, x=x)\n",
        "    next_token_logits = logits[:, -1, :]\n",
        "    next_token = jrandom.categorical(jrandom.fold_in(key, t), next_token_logits)\n",
        "    if next_token.item() == tokenizer.eos_id:\n",
        "      break\n",
        "    result.append(next_token.item())\n",
        "    x = jnp.concatenate((x, next_token.reshape(-1,1)), axis=-1)\n",
        "\n",
        "  return tokenizer.decode(jnp.array(result))\n",
        "\n",
        "result = generate(jrandom.key(0), model, tokenizer, variables, \"Citizen: \\n\", max_tokens=30)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fC9Ekvw4OTed",
        "outputId": "57c643c2-e3e7-46d6-ce85-e52a0a2df549"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Citizen: \n",
            "Further of: but'another at the gates,\n",
            "Since thou, my cousin, which one would be verified, nothing,--\n",
            "\n",
            "C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scale to LLaMA2\n",
        "\n",
        "Now let's just use the jax code to load a LLaMA checkpoint and do prediction.\n",
        "\n"
      ],
      "metadata": {
        "id": "_iomukB2WQGd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## All Model definitions\n",
        "\n",
        "For clarity and convenience, I've just collected all model definitions in one place:"
      ],
      "metadata": {
        "id": "KsnzLapRPPU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import dataclasses\n",
        "import json\n",
        "\n",
        "import rich\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp, jax.random as jrandom\n",
        "from flax import linen as nn\n",
        "from flax import traverse_util\n",
        "import einops\n",
        "\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class Config:\n",
        "  \"\"\"GPT Config.\"\"\"\n",
        "  max_seq_length: int      # maximum context length\n",
        "  vocab_size: int          # vocabulary size\n",
        "  n_embed: int             # embedding size (= n_head * head_size)\n",
        "  n_layer: int             # number of transformer blocks\n",
        "  intermediate_size: int   # intermediate size of FFN\n",
        "\n",
        "  # multi head / multi query attention\n",
        "  n_head: int              # number of heads\n",
        "  n_query_groups: int      # number of query groups in multi-query attention\n",
        "\n",
        "  # RoPE positional embedding\n",
        "  rope_condense_ratio: int # rope condense ratio\n",
        "  rope_base: int           # rope base\n",
        "\n",
        "  def __post_init__(self):\n",
        "    assert self.n_embed % self.n_head == 0, f'Embedding size(n_embed={self.n_embed}) should be divisible by (n_head={self.n_head})'\n",
        "    self.head_size = self.n_embed // self.n_head\n",
        "\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "  \"\"\"Root Mean Square Layer Normalization.\n",
        "\n",
        "  Derived from https://github.com/bzhangGo/rmsnorm/blob/master/rmsnorm_torch.py. BSD 3-Clause License:\n",
        "  https://github.com/bzhangGo/rmsnorm/blob/master/LICENSE.\n",
        "  \"\"\"\n",
        "  axis: int = -1\n",
        "  eps: float = 1e-5\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    weight = self.param('weight', lambda rng, shape: jnp.ones(shape), x.shape[-1])\n",
        "    norm_x = jnp.mean(x * x, axis=self.axis, keepdims=True)\n",
        "    x_normed = x / jnp.sqrt(norm_x + self.eps)\n",
        "    return weight * x_normed\n",
        "\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "  \"\"\"Multi head / Multi query / Grouped Query Attention.\n",
        "\n",
        "  About n_query_groups\n",
        "  to use multi-head attention (MHA), set this to `n_head` (default)\n",
        "  to use multi-query attention (MQA), set this to 1\n",
        "  to use grouped-query attention (GQA), set this to a value in between\n",
        "  Example with `n_head=4`\n",
        "  ┌───┐┌───┐┌───┐┌───┐     ┌───┐    ┌───┐             ┌───┐\n",
        "  │ v ││ v ││ v ││ v │     │ v │    │ v │             │ v │\n",
        "  └───┘└───┘└───┘└───┘     └───┘    └───┘             └───┘\n",
        "    │    │    │    │         │        │                 │\n",
        "  ┌───┐┌───┐┌───┐┌───┐     ┌───┐    ┌───┐             ┌───┐\n",
        "  │ k ││ k ││ k ││ k │     │ k │    │ k │             │ k │\n",
        "  └───┘└───┘└───┘└───┘     └───┘    └───┘             └───┘\n",
        "    │    │    │    │      ┌──┴──┐  ┌──┴──┐      ┌────┬──┴─┬────┐\n",
        "  ┌───┐┌───┐┌───┐┌───┐  ┌───┐┌───┐┌───┐┌───┐  ┌───┐┌───┐┌───┐┌───┐\n",
        "  │ q ││ q ││ q ││ q │  │ q ││ q ││ q ││ q │  │ q ││ q ││ q ││ q │\n",
        "  └───┘└───┘└───┘└───┘  └───┘└───┘└───┘└───┘  └───┘└───┘└───┘└───┘\n",
        "  ◀──────────────────▶  ◀──────────────────▶  ◀──────────────────▶\n",
        "          MHA                    GQA                   MQA\n",
        "    n_query_groups=4       n_query_groups=2      n_query_groups=1\n",
        "    q_per_kv=1             q_per_kv=2            q_per_kv=4\n",
        "    n_head=4               n_head=4              n_head=4\n",
        "    n_qkv=3                n_qkv=4               n_qkv=6\n",
        "  credit https://arxiv.org/pdf/2305.13245.pdf\n",
        "  \"\"\"\n",
        "  config: Config\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x, rope_emb, mask=None):\n",
        "    T = x.shape[-2]  # x: (B, T, C)\n",
        "    mask = mask or jnp.tril(jnp.ones((T, T)))\n",
        "\n",
        "    # nq = n_head, nk = nv = n_query_groups\n",
        "    qkv_dim = (self.config.n_head + 2 * self.config.n_query_groups) * self.config.head_size\n",
        "    qkv_proj = nn.Dense(features=qkv_dim, use_bias=False, name='proj_qkv')(x)\n",
        "\n",
        "    # number of q's per group\n",
        "    q_per_kv = self.config.n_head // self.config.n_query_groups\n",
        "    # number of qkvs per group, k=v=1\n",
        "    n_qkv = q_per_kv + 2\n",
        "    # break embedding into (n_groups, n_qkv, head_size)\n",
        "    qkv = einops.rearrange(qkv_proj, 'b t (n_groups n_qkv h) -> b n_groups n_qkv t h',\n",
        "                           n_groups=self.config.n_query_groups,\n",
        "                           n_qkv=n_qkv)\n",
        "    # split q, k, v within groups\n",
        "    q, k, v = einops.unpack(qkv, [[q_per_kv], [1], [1]], 'b n_groups * t h')\n",
        "\n",
        "    if q_per_kv != 1:\n",
        "      # repeat k and v in each group\n",
        "      k = einops.repeat(k, 'b n_groups 1 t h -> b n_groups q_per_kv t h', q_per_kv=q_per_kv)\n",
        "      v = einops.repeat(v, 'b n_groups 1 t h -> b n_groups q_per_kv t h', q_per_kv=q_per_kv)\n",
        "\n",
        "    # merge groups into heads\n",
        "    q = einops.rearrange(q, 'b n_groups q_per_kv t h -> b (n_groups q_per_kv) t h')\n",
        "    k = einops.rearrange(k, 'b n_groups q_per_kv t h -> b (n_groups q_per_kv) t h')\n",
        "    v = einops.rearrange(v, 'b n_groups q_per_kv t h -> b (n_groups q_per_kv) t h')\n",
        "\n",
        "    # apply position embedding\n",
        "    # NOTE: only apply to q and k, but not v\n",
        "    q = apply_rope(q, rope_emb)\n",
        "    k = apply_rope(k, rope_emb)\n",
        "\n",
        "    # multi head scaled dot attention\n",
        "    weights = einops.einsum(q, k, 'b nh tq h, b nh tk h -> b nh tq tk')\n",
        "    weights = weights / jnp.sqrt(self.config.head_size)\n",
        "    weights = jnp.where(mask, weights, float('-inf'))\n",
        "    weights = nn.softmax(weights, axis=-1)\n",
        "    out = einops.einsum(weights, v, 'b nh tq tv, b nh tv h -> b tq nh h')\n",
        "\n",
        "    # concat heads\n",
        "    out = einops.rearrange(out, 'b t nh h -> b t (nh h)')\n",
        "\n",
        "    # final projection\n",
        "    out = nn.Dense(self.config.n_embed, use_bias=False, name='proj_out')(out)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  \"\"\"LLaMA style MLP.\"\"\"\n",
        "  config: Config\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    x1 = nn.Dense(self.config.intermediate_size, use_bias=False, name='fc_1')(x)\n",
        "    x2 = nn.Dense(self.config.intermediate_size, use_bias=False, name='fc_2')(x)\n",
        "    x = nn.silu(x1) * x2\n",
        "    x = nn.Dense(self.config.n_embed, use_bias=False, name='proj_out')(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "  \"\"\"A Transformer Block of attention followed by MLP.\"\"\"\n",
        "  config: Config\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x, rope_emb, mask=None):\n",
        "    n1 = RMSNorm(name='norm_1')(x)\n",
        "    h = SelfAttention(self.config, name='attn')(n1, rope_emb, mask=mask)\n",
        "    x = h + x\n",
        "    n2 = RMSNorm(name='norm_2')(x)\n",
        "    h = MLP(self.config, name='mlp')(n2)\n",
        "    x = h + x\n",
        "    return x\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "  \"\"\"The full decoder only tranformer.\"\"\"\n",
        "\n",
        "  config: Config\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    T = x.shape[-1]  # (B, T)\n",
        "    rope_emb = self.variable('cache', 'rope_emb', build_rope_cache,\n",
        "                             self.config.max_seq_length,\n",
        "                             self.config.head_size,\n",
        "                             self.config.rope_base,\n",
        "                             self.config.rope_condense_ratio)\n",
        "\n",
        "    x = nn.Embed(num_embeddings=self.config.vocab_size,\n",
        "                 features=self.config.n_embed, name='emb')(x)\n",
        "    self.sow('intermediates', 'emb_out', x)\n",
        "\n",
        "    for i in range(self.config.n_layer):\n",
        "      x = Block(config=self.config, name=f'block_{i}')(x, rope_emb.value[:T])\n",
        "      self.sow('intermediates', f'block_{i}_out', x)\n",
        "\n",
        "    # final layer norm\n",
        "    x = RMSNorm(name='ln_f')(x)\n",
        "    self.sow('intermediates', 'ln_out', x)\n",
        "    # language model head\n",
        "    x = nn.Dense(self.config.vocab_size, name='lm_head', use_bias=False)(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def build_rope_cache(\n",
        "    seq_len: int,\n",
        "    n_elem: int,\n",
        "    base: int = 10000,\n",
        "    condense_ratio: int = 1\n",
        "):\n",
        "  \"\"\"Enhanced Transformer with Rotary Position Embedding.\n",
        "\n",
        "  Derived from: https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/\n",
        "  transformers/rope/__init__.py. MIT License:\n",
        "  https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/license.\n",
        "  \"\"\"\n",
        "  # $\\Theta = {\\theta_i = 10000^{\\frac{2(i-1)}{d}}, i \\in [1, 2, ..., \\frac{d}{2}]}$\n",
        "  theta = 1.0 / (base ** (jnp.arange(0, n_elem, 2) / n_elem))\n",
        "\n",
        "  # Create position indexes `[0, 1, ..., seq_len - 1]`\n",
        "  seq_idx = jnp.arange(seq_len) / condense_ratio\n",
        "\n",
        "  # Calculate the product of position index and $\\theta_i$\n",
        "  idx_theta = jnp.outer(seq_idx, theta)\n",
        "  idx_theta = jnp.tile(idx_theta, (1, 2))\n",
        "\n",
        "  return jnp.stack([jnp.cos(idx_theta), jnp.sin(idx_theta)], axis=-1)\n",
        "\n",
        "\n",
        "def apply_rope(x, rope_emb):\n",
        "  \"\"\"Apply rope embedding to input x.\"\"\"\n",
        "  cos, sin = rope_emb[..., 0], rope_emb[..., 1]\n",
        "  head_size = x.shape[-1]\n",
        "  x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)\n",
        "  x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)\n",
        "  rotated = jnp.concatenate((-x2, x1), axis=-1)  # (B, nh, T, hs)\n",
        "  roped = (x * cos) + (rotated * sin)\n",
        "  return roped\n"
      ],
      "metadata": {
        "id": "8njaJtLKvHhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert pytorch checkpoint to Jax"
      ],
      "metadata": {
        "id": "y5JKFraUPD1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import torch\n",
        "\n",
        "def init_from_llama_checkpoint(\n",
        "    model: GPT,\n",
        "    config: Config,\n",
        "    llama_checkpoint: Path):\n",
        "  variable_shapes = jax.eval_shape(model.init, jrandom.key(0), jnp.zeros((1,1), dtype=jnp.int32))\n",
        "  param_shapes = traverse_util.flatten_dict(variable_shapes['params'], sep='.')\n",
        "\n",
        "  if not llama_checkpoint.is_dir():\n",
        "    raise ValueError(f'llama checkpoint directory {llama_checkpoint} does not exist')\n",
        "\n",
        "  with open(llama_checkpoint / 'pytorch_model.bin.index.json', 'r') as f:\n",
        "    llama_model_index = json.load(f)\n",
        "\n",
        "  param_names = sorted(llama_model_index['weight_map'].keys())\n",
        "  files = sorted(list(set(llama_model_index['weight_map'].values())))\n",
        "\n",
        "  params = {}\n",
        "\n",
        "  def load_params(name, value):\n",
        "    if hasattr(value, 'numpy'):\n",
        "      value = value.numpy()\n",
        "    assert name in param_shapes, f'Param does not exist: {name}'\n",
        "    assert param_shapes[name].shape == value.shape, f'Shapes not match: {name} Expected: {param_shapes[name].shape} Actual: {value.shape}'\n",
        "    params[name] = jax.device_put(value)\n",
        "    print(f'Success: loaded param: {name}, dtype:{params[name].dtype} shape:{params[name].shape} device:{params[name].device()}')\n",
        "\n",
        "  qkv = {i: {} for i in range(config.n_layer)}\n",
        "\n",
        "  for f in files:\n",
        "    print(f'Loading checkpoint file {f}')\n",
        "    states = torch.load(llama_checkpoint / f)\n",
        "    for name, value in states.items():\n",
        "      if name == 'lm_head.weight':\n",
        "        load_params('lm_head.kernel', value.T)\n",
        "      elif name == 'model.embed_tokens.weight':\n",
        "        load_params('emb.embedding', value)\n",
        "      elif name == 'model.norm.weight':\n",
        "        load_params('ln_f.weight', value)\n",
        "      elif ret := re.match(r'model.layers\\.(\\d+)\\.(.*)', name):\n",
        "        i, sub_name = ret.groups()\n",
        "        i = int(i)\n",
        "        if sub_name == 'input_layernorm.weight':\n",
        "          load_params(f'block_{i}.norm_1.weight', value)\n",
        "        elif sub_name == 'post_attention_layernorm.weight':\n",
        "          load_params(f'block_{i}.norm_2.weight', value)\n",
        "        elif sub_name == 'mlp.gate_proj.weight':\n",
        "          load_params(f'block_{i}.mlp.fc_1.kernel', value.T)\n",
        "        elif sub_name == 'mlp.up_proj.weight':\n",
        "          load_params(f'block_{i}.mlp.fc_2.kernel', value.T)\n",
        "        elif sub_name == 'mlp.down_proj.weight':\n",
        "          load_params(f'block_{i}.mlp.proj_out.kernel', value.T)\n",
        "        elif sub_name == 'self_attn.o_proj.weight':\n",
        "          load_params(f'block_{i}.attn.proj_out.kernel', value.T)\n",
        "        elif sub_name == 'self_attn.q_proj.weight':\n",
        "          qkv[i]['q'] = value.numpy()\n",
        "        elif sub_name == 'self_attn.k_proj.weight':\n",
        "          qkv[i]['k'] = value.numpy()\n",
        "        elif sub_name == 'self_attn.v_proj.weight':\n",
        "          qkv[i]['v'] = value.numpy()\n",
        "        elif sub_name == 'self_attn.rotary_emb.inv_freq':\n",
        "          pass\n",
        "        else:\n",
        "          raise ValueError(f'unhandled param: {name}')\n",
        "      else:\n",
        "        raise ValueError(f'unhandled param: {name}')\n",
        "    del(states)  # save memory\n",
        "\n",
        "  def combine_qkv(q, k, v, n_heads):\n",
        "    q = einops.rearrange(q, '(nh h) n_embed -> n_embed nh 1 h', nh=n_heads)\n",
        "    k = einops.rearrange(k, '(nh h) n_embed -> n_embed nh 1 h', nh=n_heads)\n",
        "    v = einops.rearrange(v, '(nh h) n_embed -> n_embed nh 1 h', nh=n_heads)\n",
        "\n",
        "    packed, _ = einops.pack([q,k,v], 'n_embed nh * h')\n",
        "    qkv = einops.rearrange(packed, 'n_embed nh n_qkv h -> n_embed (nh n_qkv h)')\n",
        "    return qkv\n",
        "\n",
        "  for i in range(config.n_layer):\n",
        "    q, k, v = qkv[i]['q'], qkv[i]['k'], qkv[i]['v']\n",
        "    proj_qkv = combine_qkv(q, k, v, config.n_head)\n",
        "    load_params(f'block_{i}.attn.proj_qkv.kernel', proj_qkv)\n",
        "\n",
        "  return traverse_util.unflatten_dict(params, sep='.')\n",
        "\n",
        "\n",
        "# deduced from: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/tree/main\n",
        "llama2_7b_config = Config(\n",
        "    max_seq_length=4096,\n",
        "    vocab_size=32000,\n",
        "    n_layer=32,\n",
        "    n_head=32,\n",
        "    n_embed=4096,\n",
        "    n_query_groups=32,\n",
        "    intermediate_size=11008,\n",
        "    rope_base=10000,\n",
        "    rope_condense_ratio=1,\n",
        ")\n",
        "\n",
        "model = GPT(llama2_7b_config)\n",
        "llama_checkpoint = Path('/content/drive/MyDrive/checkpoints/meta-llama/Llama-2-7b-chat-hf')\n",
        "params = init_from_llama_checkpoint(model, llama2_7b_config, llama_checkpoint)\n",
        "\n",
        "variables = model.init(jrandom.key(0), jnp.zeros((1,1), dtype=jnp.int32))\n",
        "del(variables['params'])\n",
        "variables['params'] = params\n",
        "\n",
        "# rich.print(variables)"
      ],
      "metadata": {
        "id": "jpMBbquJzuJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we save using orbax so we don't have to load the pytorch checkpoint next time:"
      ],
      "metadata": {
        "id": "rgRCxd-CICPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flax.training import orbax_utils\n",
        "import orbax.checkpoint\n",
        "import json\n",
        "\n",
        "def save_checkpoint(variables, path: Path):\n",
        "  orbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
        "  model_index = {}\n",
        "  for name, value in traverse_util.flatten_dict(variables, sep='.').items():\n",
        "    ckpt = {'value': value}\n",
        "    orbax_checkpointer.save(path / name, ckpt, save_args=orbax_utils.save_args_from_target(ckpt))\n",
        "    model_index[name] = 'true'\n",
        "    print(f'Saved {name}')\n",
        "\n",
        "  with open(path / 'model_index.json', 'w') as f:\n",
        "    json.dump(model_index, f)\n",
        "\n",
        "  print(f'Save success')\n",
        "\n",
        "save_checkpoint(variables, Path('/content/drive/MyDrive/checkpoints/flaxgpt-llama-2-7b-hf-chat'))"
      ],
      "metadata": {
        "id": "yX9rUoxsGBCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load jax checkpoint"
      ],
      "metadata": {
        "id": "L7KTSzj4ILcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flax.training import orbax_utils\n",
        "import orbax.checkpoint\n",
        "import json\n",
        "\n",
        "def load_checkpoint(path: Path):\n",
        "  with open(path / 'model_index.json') as f:\n",
        "    model_index = json.load(f)\n",
        "\n",
        "  orbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
        "\n",
        "  variables = {}\n",
        "  for name in model_index:\n",
        "    ckpt = orbax_checkpointer.restore(path / name)\n",
        "    variables[name] = jax.device_put(ckpt['value'])\n",
        "    print(f'Loaded variable: {name}')\n",
        "\n",
        "  return traverse_util.unflatten_dict(variables, sep='.')"
      ],
      "metadata": {
        "id": "Dm-erb0hM1U0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate using loaded jax checkpoint"
      ],
      "metadata": {
        "id": "OYjFwojjO4Pf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from logging import getLogger\n",
        "from typing import List\n",
        "\n",
        "from sentencepiece import SentencePieceProcessor\n",
        "\n",
        "\n",
        "logger = getLogger()\n",
        "\n",
        "\n",
        "class Tokenizer:\n",
        "    \"\"\"tokenizing and encoding/decoding text using SentencePiece.\"\"\"\n",
        "    def __init__(self, model_path: str):\n",
        "        \"\"\"\n",
        "        Initializes the Tokenizer with a SentencePiece model.\n",
        "\n",
        "        Args:\n",
        "            model_path (str): The path to the SentencePiece model file.\n",
        "        \"\"\"\n",
        "        # reload tokenizer\n",
        "        assert os.path.isfile(model_path), model_path\n",
        "        self.sp_model = SentencePieceProcessor(model_file=model_path)\n",
        "        logger.info(f\"Reloaded SentencePiece model from {model_path}\")\n",
        "\n",
        "        # BOS / EOS token IDs\n",
        "        self.n_words: int = self.sp_model.vocab_size()\n",
        "        self.bos_id: int = self.sp_model.bos_id()\n",
        "        self.eos_id: int = self.sp_model.eos_id()\n",
        "        self.pad_id: int = self.sp_model.pad_id()\n",
        "        logger.info(\n",
        "            f\"#words: {self.n_words} - BOS ID: {self.bos_id} - EOS ID: {self.eos_id}\"\n",
        "        )\n",
        "        assert self.sp_model.vocab_size() == self.sp_model.get_piece_size()\n",
        "\n",
        "    def encode(self, s: str, bos: bool, eos: bool) -> List[int]:\n",
        "        \"\"\"\n",
        "        Encodes a string into a list of token IDs.\n",
        "\n",
        "        Args:\n",
        "            s (str): The input string to be encoded.\n",
        "            bos (bool): Whether to prepend the beginning-of-sequence token.\n",
        "            eos (bool): Whether to append the end-of-sequence token.\n",
        "\n",
        "        Returns:\n",
        "            List[int]: A list of token IDs.\n",
        "        \"\"\"\n",
        "        assert type(s) is str\n",
        "        t = self.sp_model.encode(s)\n",
        "        if bos:\n",
        "            t = [self.bos_id] + t\n",
        "        if eos:\n",
        "            t = t + [self.eos_id]\n",
        "        return t\n",
        "\n",
        "    def decode(self, t: List[int]) -> str:\n",
        "        \"\"\"\n",
        "        Decodes a list of token IDs into a string.\n",
        "\n",
        "        Args:\n",
        "            t (List[int]): The list of token IDs to be decoded.\n",
        "\n",
        "        Returns:\n",
        "            str: The decoded string.\n",
        "        \"\"\"\n",
        "        return self.sp_model.decode(t)\n",
        "\n",
        "\n",
        "def generate(key, model, tokenizer, variables, prompt, max_tokens=100):\n",
        "  x = jnp.array(tokenizer.encode(prompt, bos=True, eos=False)).reshape(1, -1)\n",
        "  result = x[0].tolist()\n",
        "\n",
        "  for t in range(max_tokens):\n",
        "    x = x[..., -model.config.max_seq_length:]\n",
        "    logits = model.apply(variables, x=x)\n",
        "    next_token_logits = logits[:, -1, :]\n",
        "    next_token = jrandom.categorical(jrandom.fold_in(key, t), next_token_logits)\n",
        "    print(f't={t}', tokenizer.decode(result))\n",
        "    if next_token.item() == tokenizer.eos_id:\n",
        "      break\n",
        "    result.append(next_token.item())\n",
        "    x = jnp.concatenate((x, next_token.reshape(-1,1)), axis=-1)\n",
        "\n",
        "  return tokenizer.decode(result)\n"
      ],
      "metadata": {
        "id": "hAFN8d1gDYzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llama_checkpoint = llama_checkpoint = Path('/content/drive/MyDrive/checkpoints/meta-llama/Llama-2-7b-chat-hf')\n",
        "tokenizer = Tokenizer(model_path=str(llama_checkpoint / 'tokenizer.model'))\n",
        "variables = load_checkpoint(Path('/content/drive/MyDrive/checkpoints/flaxgpt-llama-2-7b-hf-chat'))\n",
        "\n",
        "# deduced from: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/tree/main\n",
        "llama2_7b_config = Config(\n",
        "    max_seq_length=4096,\n",
        "    vocab_size=32000,\n",
        "    n_layer=32,\n",
        "    n_head=32,\n",
        "    n_embed=4096,\n",
        "    n_query_groups=32,\n",
        "    intermediate_size=11008,\n",
        "    rope_base=10000,\n",
        "    rope_condense_ratio=1,\n",
        ")\n",
        "\n",
        "model = GPT(llama2_7b_config)\n",
        "\n",
        "r = generate(jrandom.PRNGKey(0), model, tokenizer, variables, 'Hello, my name is', max_tokens=10)\n",
        "print(r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNUfHnZ8Whnx",
        "outputId": "e119d312-78fb-4ca5-fcc9-7f6a215a8134"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded variable: cache.rope_emb\n",
            "Loaded variable: params.emb.embedding\n",
            "Loaded variable: params.block_0.attn.proj_out.kernel\n",
            "Loaded variable: params.block_0.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_0.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_0.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_0.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_0.norm_1.weight\n",
            "Loaded variable: params.block_0.norm_2.weight\n",
            "Loaded variable: params.block_1.attn.proj_out.kernel\n",
            "Loaded variable: params.block_1.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_1.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_1.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_1.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_1.norm_1.weight\n",
            "Loaded variable: params.block_1.norm_2.weight\n",
            "Loaded variable: params.block_2.attn.proj_out.kernel\n",
            "Loaded variable: params.block_2.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_2.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_2.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_2.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_2.norm_1.weight\n",
            "Loaded variable: params.block_2.norm_2.weight\n",
            "Loaded variable: params.block_3.attn.proj_out.kernel\n",
            "Loaded variable: params.block_3.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_3.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_3.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_3.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_3.norm_1.weight\n",
            "Loaded variable: params.block_3.norm_2.weight\n",
            "Loaded variable: params.block_4.attn.proj_out.kernel\n",
            "Loaded variable: params.block_4.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_4.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_4.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_4.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_4.norm_1.weight\n",
            "Loaded variable: params.block_4.norm_2.weight\n",
            "Loaded variable: params.block_5.attn.proj_out.kernel\n",
            "Loaded variable: params.block_5.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_5.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_5.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_5.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_5.norm_1.weight\n",
            "Loaded variable: params.block_5.norm_2.weight\n",
            "Loaded variable: params.block_6.attn.proj_out.kernel\n",
            "Loaded variable: params.block_6.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_6.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_6.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_6.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_6.norm_1.weight\n",
            "Loaded variable: params.block_6.norm_2.weight\n",
            "Loaded variable: params.block_7.attn.proj_out.kernel\n",
            "Loaded variable: params.block_7.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_7.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_7.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_7.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_7.norm_1.weight\n",
            "Loaded variable: params.block_7.norm_2.weight\n",
            "Loaded variable: params.block_8.attn.proj_out.kernel\n",
            "Loaded variable: params.block_8.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_8.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_8.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_8.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_8.norm_1.weight\n",
            "Loaded variable: params.block_8.norm_2.weight\n",
            "Loaded variable: params.block_9.attn.proj_out.kernel\n",
            "Loaded variable: params.block_9.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_9.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_9.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_9.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_9.norm_1.weight\n",
            "Loaded variable: params.block_9.norm_2.weight\n",
            "Loaded variable: params.block_10.attn.proj_out.kernel\n",
            "Loaded variable: params.block_10.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_10.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_10.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_10.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_10.norm_1.weight\n",
            "Loaded variable: params.block_10.norm_2.weight\n",
            "Loaded variable: params.block_11.attn.proj_out.kernel\n",
            "Loaded variable: params.block_11.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_11.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_11.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_11.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_11.norm_1.weight\n",
            "Loaded variable: params.block_11.norm_2.weight\n",
            "Loaded variable: params.block_12.attn.proj_out.kernel\n",
            "Loaded variable: params.block_12.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_12.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_12.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_12.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_12.norm_1.weight\n",
            "Loaded variable: params.block_12.norm_2.weight\n",
            "Loaded variable: params.block_13.attn.proj_out.kernel\n",
            "Loaded variable: params.block_13.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_13.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_13.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_13.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_13.norm_1.weight\n",
            "Loaded variable: params.block_13.norm_2.weight\n",
            "Loaded variable: params.block_14.attn.proj_out.kernel\n",
            "Loaded variable: params.block_14.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_14.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_14.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_14.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_14.norm_1.weight\n",
            "Loaded variable: params.block_14.norm_2.weight\n",
            "Loaded variable: params.block_15.attn.proj_out.kernel\n",
            "Loaded variable: params.block_15.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_15.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_15.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_15.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_15.norm_1.weight\n",
            "Loaded variable: params.block_15.norm_2.weight\n",
            "Loaded variable: params.block_16.attn.proj_out.kernel\n",
            "Loaded variable: params.block_16.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_16.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_16.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_16.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_16.norm_1.weight\n",
            "Loaded variable: params.block_16.norm_2.weight\n",
            "Loaded variable: params.block_17.attn.proj_out.kernel\n",
            "Loaded variable: params.block_17.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_17.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_17.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_17.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_17.norm_1.weight\n",
            "Loaded variable: params.block_17.norm_2.weight\n",
            "Loaded variable: params.block_18.attn.proj_out.kernel\n",
            "Loaded variable: params.block_18.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_18.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_18.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_18.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_18.norm_1.weight\n",
            "Loaded variable: params.block_18.norm_2.weight\n",
            "Loaded variable: params.block_19.attn.proj_out.kernel\n",
            "Loaded variable: params.block_19.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_19.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_19.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_19.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_19.norm_1.weight\n",
            "Loaded variable: params.block_19.norm_2.weight\n",
            "Loaded variable: params.block_20.attn.proj_out.kernel\n",
            "Loaded variable: params.block_20.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_20.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_20.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_20.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_20.norm_1.weight\n",
            "Loaded variable: params.block_20.norm_2.weight\n",
            "Loaded variable: params.block_21.attn.proj_out.kernel\n",
            "Loaded variable: params.block_21.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_21.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_21.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_21.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_21.norm_1.weight\n",
            "Loaded variable: params.block_21.norm_2.weight\n",
            "Loaded variable: params.block_22.attn.proj_out.kernel\n",
            "Loaded variable: params.block_22.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_22.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_22.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_22.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_22.norm_1.weight\n",
            "Loaded variable: params.block_22.norm_2.weight\n",
            "Loaded variable: params.block_23.attn.proj_out.kernel\n",
            "Loaded variable: params.block_23.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_23.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_23.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_23.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_23.norm_1.weight\n",
            "Loaded variable: params.block_23.norm_2.weight\n",
            "Loaded variable: params.block_24.attn.proj_out.kernel\n",
            "Loaded variable: params.block_24.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_24.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_24.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_24.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_24.norm_1.weight\n",
            "Loaded variable: params.block_24.norm_2.weight\n",
            "Loaded variable: params.block_25.attn.proj_out.kernel\n",
            "Loaded variable: params.block_25.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_25.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_25.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_25.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_25.norm_1.weight\n",
            "Loaded variable: params.block_25.norm_2.weight\n",
            "Loaded variable: params.block_26.attn.proj_out.kernel\n",
            "Loaded variable: params.block_26.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_26.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_26.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_26.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_26.norm_1.weight\n",
            "Loaded variable: params.block_26.norm_2.weight\n",
            "Loaded variable: params.block_27.attn.proj_out.kernel\n",
            "Loaded variable: params.block_27.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_27.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_27.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_27.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_27.norm_1.weight\n",
            "Loaded variable: params.block_27.norm_2.weight\n",
            "Loaded variable: params.block_28.attn.proj_out.kernel\n",
            "Loaded variable: params.block_28.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_28.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_28.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_28.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_28.norm_1.weight\n",
            "Loaded variable: params.block_28.norm_2.weight\n",
            "Loaded variable: params.block_29.attn.proj_out.kernel\n",
            "Loaded variable: params.block_29.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_29.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_29.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_29.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_29.norm_1.weight\n",
            "Loaded variable: params.block_29.norm_2.weight\n",
            "Loaded variable: params.block_30.attn.proj_out.kernel\n",
            "Loaded variable: params.block_30.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_30.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_30.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_30.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_30.norm_1.weight\n",
            "Loaded variable: params.block_30.norm_2.weight\n",
            "Loaded variable: params.block_31.attn.proj_out.kernel\n",
            "Loaded variable: params.block_31.attn.proj_qkv.kernel\n",
            "Loaded variable: params.block_31.mlp.fc_1.kernel\n",
            "Loaded variable: params.block_31.mlp.fc_2.kernel\n",
            "Loaded variable: params.block_31.mlp.proj_out.kernel\n",
            "Loaded variable: params.block_31.norm_1.weight\n",
            "Loaded variable: params.block_31.norm_2.weight\n",
            "Loaded variable: params.ln_f.weight\n",
            "Loaded variable: params.lm_head.kernel\n",
            "t=0 Hello, my name is\n",
            "t=1 Hello, my name is [\n",
            "t=2 Hello, my name is [Your\n",
            "t=3 Hello, my name is [Your Name\n",
            "t=4 Hello, my name is [Your Name]\n",
            "t=5 Hello, my name is [Your Name] and\n",
            "t=6 Hello, my name is [Your Name] and I\n",
            "t=7 Hello, my name is [Your Name] and I am\n",
            "t=8 Hello, my name is [Your Name] and I am a\n",
            "t=9 Hello, my name is [Your Name] and I am a student\n",
            "Hello, my name is [Your Name] and I am a student at\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4j9kC_xuQCw4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}