{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# GPT from Scratch in Jax\n",
        "\n",
        "TLDR: I'm reproducing the GPT transformer from this video [Let's build GPT: from scratch, in code, spelled out.\n",
        "](https://www.youtube.com/watch?v=kCc8FmEb1nY) by Andrej Karpathy.\n",
        "\n",
        "The original one was trained using pytorch, but I'm using Jax."
      ],
      "metadata": {
        "id": "nl5Nqw7M9UE0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Some Notes about Jax\n",
        "\n",
        "## Jax vs Pytorch differences\n",
        "\n",
        "- Gradients are computed explicitly by ```grad(fn)(params)```\n",
        "  - No `torch.no_grad()`. no gradient computed by default.\n",
        "  - No `zero_gradient` and `loss.backward()`\n",
        "  - You don't need to put model into `training` or `eval` modes.\n",
        "- All states (params and optimizer states) are explicit and immutable.\n",
        "- Model can be defined to work with a single example instead of a mini-batch.   \n",
        "  - Use `vmap` to transform it to a batched version.\n",
        "  - This is not mandatory; you can still define models to work with only batched data.\n",
        "- Random numbers are explicit and deterministic\n",
        "\n",
        "## GPU preallocation\n",
        "Don't panic if the GPU memory usage suddenly goes up.\n",
        "\n",
        "JAX will [preallocate](https://jax.readthedocs.io/en/latest/gpu_memory_allocation.html) 75% of the total GPU memory when the first JAX operation is run."
      ],
      "metadata": {
        "id": "1u_cJbhuWjP4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset and Task\n",
        "\n",
        "We are building a character-based language model from the tiny shakespeare dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "cXU0tBKScUgs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tokenization\n",
        "\n",
        "The tokenization for char-based LMs are straightforward: just map chars to integers and back."
      ],
      "metadata": {
        "id": "UgVJcmstnLUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from functools import partial\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import jax\n",
        "import optax\n",
        "from jax import jit, nn, vmap, grad, value_and_grad, device_put\n",
        "from jax import numpy as jnp, random as jrandom\n",
        "\n",
        "\n",
        "if not os.path.exists('input.txt'):\n",
        "  !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "  print('data downloaded')\n",
        "with open('input.txt', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print('vocab size:', vocab_size, 'vocabulary:', ''.join(chars), )\n",
        "\n",
        "char_to_idx = {c: i for i, c in enumerate(chars)}\n",
        "idx_to_char = {i: c for i, c in enumerate(chars)}\n",
        "\n",
        "def encode(text):\n",
        "  return [char_to_idx[c] for c in text]\n",
        "\n",
        "def decode(indices):\n",
        "  return ''.join([idx_to_char[i] for i in indices])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tyrn2dg0cDQg",
        "outputId": "e4bfe2bc-bc40-421f-d9de-90d354486f63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size: 65 vocabulary: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## jax dataloader, version 1"
      ],
      "metadata": {
        "id": "qevTHAJAm4lR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %90 train data, 10% validation data\n",
        "data = jnp.array(encode(text), dtype=jnp.int32)\n",
        "n = int(len(data)*0.9)\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "\n",
        "def get_batch(key, split, context_length=8, batch_size=32):\n",
        "  \"\"\"get a random batch of split of train or val data.\"\"\"\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  idx = jrandom.randint(key, minval=0, maxval=len(data)-context_length, shape=(batch_size,))\n",
        "  xb = jnp.stack([data[i: i+context_length] for i in idx])\n",
        "  yb = jnp.stack([data[i+1:i+1+context_length] for i in idx])\n",
        "  return xb, yb\n",
        "\n",
        "xb, yb = get_batch(jrandom.PRNGKey(0), 'train')\n",
        "print(f'xb: {xb.shape}, yb: {yb.shape}')\n",
        "print('decoded x:', decode(xb[0].tolist()), ',y:', decode(yb[0].tolist()))\n",
        "del xb, yb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xu1sY7kTl9c5",
        "outputId": "7e14db4d-4657-4a61-c08a-941ef6829c7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xb: (32, 8), yb: (32, 8)\n",
            "decoded x: she's at ,y: he's at \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function above looks right (and in fact is), but the problem is that it is extremely slow. Since I didn't find the speed problem until I started training, it took me quite some time to realize it was because of the data loading, not the actual model training."
      ],
      "metadata": {
        "id": "XLMv3G_QtXrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit get_batch(jrandom.PRNGKey(0), 'train')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-EZWWXJsi2y",
        "outputId": "2c9bc947-62df-4a1c-bfb5-17ffd9927b73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "436 ms ± 161 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## jax dataloader, version 2"
      ],
      "metadata": {
        "id": "1mM5fNGdoo_3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The orginal torch version, although looking very similar, was much faster:"
      ],
      "metadata": {
        "id": "Hh6MhqTwvSC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "def get_batch(split, context_length=8, batch_size=32):\n",
        "  \"\"\"get a random batch of split of train or val data.\"\"\"\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  idx = torch.randint(len(data) - context_length, (batch_size,))\n",
        "  xb = torch.stack([data[i: i+context_length] for i in idx])\n",
        "  yb = torch.stack([data[i+1:i+1+context_length] for i in idx])\n",
        "  return xb, yb\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print(f'xb: {xb.shape}, yb: {yb.shape}')\n",
        "print('decoded x:', decode(xb[0].tolist()), ',y:', decode(yb[0].tolist()))\n",
        "del xb, yb\n",
        "\n",
        "\n",
        "%timeit get_batch('train')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOXBabIJvV2y",
        "outputId": "94ea1a39-8815-4780-f60d-dbda9e404ec6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xb: torch.Size([32, 8]), yb: torch.Size([32, 8])\n",
            "decoded x: he gives ,y: e gives \n",
            "796 µs ± 8.35 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I tried different ways, the closes I could get with Jax was this:"
      ],
      "metadata": {
        "id": "ssn8eBXXykK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = jnp.array(encode(text), dtype=jnp.int32)\n",
        "n = int(len(data)*0.9)\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "\n",
        "def get_batch(key, split, context_length=8, batch_size=32):\n",
        "  \"\"\"get a random batch of split of train or val data.\"\"\"\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  idx = jrandom.randint(key, minval=0, maxval=len(data)-context_length, shape=(batch_size,))\n",
        "  idx_batch = idx[...,None] + jnp.arange(context_length)\n",
        "  xb, yb = data[idx_batch], data[idx_batch+1]\n",
        "  return xb, yb\n",
        "\n",
        "\n",
        "xb, yb = get_batch(jrandom.PRNGKey(0), 'train')\n",
        "print(f'xb: {xb.shape}, yb: {yb.shape}')\n",
        "print('decoded x:', decode(xb[0].tolist()), ', y:', decode(yb[0].tolist()))\n",
        "del xb, yb\n",
        "\n",
        "%timeit get_batch(jrandom.PRNGKey(0), 'train')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BP53-qBsqZh",
        "outputId": "6a85885d-e026-463a-85eb-c072015fc935"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xb: (32, 8), yb: (32, 8)\n",
            "decoded x: she's at , y: he's at \n",
            "8.67 ms ± 1.32 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## numpy dataloader\n",
        "\n",
        "The best solution I found was to just use numpy for data loading:"
      ],
      "metadata": {
        "id": "aqtjaXB7zrua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "data = np.array(encode(text), dtype=np.int32)\n",
        "n = int(len(data)*0.9)\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "def get_batch(split, context_length=8, batch_size=32):\n",
        "  \"\"\"get a random batch of split of train or val data.\"\"\"\n",
        "  data = data = train_data if split == 'train' else val_data\n",
        "  idx = np.random.randint(low=0, high=len(data)-context_length, size=(batch_size,))\n",
        "  xb = np.stack([data[i: i+context_length] for i in idx])\n",
        "  yb = np.stack([data[i+1:i+1+context_length] for i in idx])\n",
        "  return jnp.array(xb), jnp.array(yb)\n"
      ],
      "metadata": {
        "id": "NrHYWt98ysKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xb, yb = get_batch('train')\n",
        "print(f'xb: {xb.shape}, yb: {yb.shape}')\n",
        "print('decoded x:', decode(xb[0].tolist()), ',y:', decode(yb[0].tolist()))\n",
        "del xb, yb\n",
        "\n",
        "%timeit get_batch('train')"
      ],
      "metadata": {
        "id": "qKgFhwcfVdH2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c76e69d1-089d-4ff3-d36a-09d5c4d1187c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xb: (32, 8), yb: (32, 8)\n",
            "decoded x: er hand. ,y: r hand.\n",
            "\n",
            "1.29 ms ± 382 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why the jax version was slow"
      ],
      "metadata": {
        "id": "VrJcYo1zozsv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The root cause was, for some reason, Jax was much slower at generating random integers:"
      ],
      "metadata": {
        "id": "goXAjCuD4i-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit np.random.randint(low=0, high=100000, size=(32,))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bk4Qota1cSyx",
        "outputId": "a517772e-35e6-4777-b6c0-e5d2db948edf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16 µs ± 6.12 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit torch.randint(100000, (32,))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ca9ECUh4MFi",
        "outputId": "9b9bfd74-f3cf-4489-84a8-3b8fda1298a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.96 µs ± 797 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k = jrandom.PRNGKey(0)\n",
        "%timeit jrandom.randint(k, minval=0, maxval=100000, shape=(32,))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5HkTgod4TeL",
        "outputId": "a533a49b-01b4-4862-fd59-4d4997adb627"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "501 µs ± 111 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## jax dataloader, super fast version\n",
        "\n",
        "The problem can be solved by jitting the function:"
      ],
      "metadata": {
        "id": "u7sRvulrtLVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = jrandom.PRNGKey(0)\n",
        "@jit\n",
        "def gen_num(key):\n",
        "  return jrandom.randint(k, minval=0, maxval=100000, shape=(32,))\n",
        "\n",
        "gen_num(k); # skip compilation\n",
        "\n",
        "%timeit gen_num(k)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3aUxI5iq3Uf",
        "outputId": "ed37a45f-97bf-4d94-da72-ee7f5c1e7ff2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "76.9 µs ± 2.22 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a jax dataloader using jit:\n",
        "\n"
      ],
      "metadata": {
        "id": "aztujutfttJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %90 train data, 10% validation data\n",
        "data = jnp.array(encode(text), dtype=jnp.int32)\n",
        "n = int(len(data)*0.9)\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "\n",
        "@partial(jit, static_argnames=['split', 'context_length', 'batch_size'])\n",
        "def get_batch(key, split, context_length=8, batch_size=32):\n",
        "  \"\"\"get a random batch of split of train or val data.\"\"\"\n",
        "  print(f'load {split} data')\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  idx = jrandom.randint(key, minval=0, maxval=len(data)-context_length, shape=(batch_size,))\n",
        "  idx_batch = idx[...,None] + jnp.arange(context_length)\n",
        "  xb, yb = data[idx_batch], data[idx_batch+1]\n",
        "  print(xb.shape, yb.shape)\n",
        "  return device_put(xb), device_put(yb)"
      ],
      "metadata": {
        "id": "bZ0nJsK2ravu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xb, yb = get_batch(jrandom.PRNGKey(0), 'train')\n",
        "print(f'xb: {xb.shape}, yb: {yb.shape}')\n",
        "print('decoded x:', decode(xb[0].tolist()), ',y:', decode(yb[0].tolist()))\n",
        "del xb, yb\n",
        "\n",
        "k = jrandom.PRNGKey(0)\n",
        "%timeit get_batch(k, 'train')"
      ],
      "metadata": {
        "id": "WfO_PNYDS4gl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf9f7ead-099b-4b86-c28c-6a583ae15107"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load train data\n",
            "(32, 8) (32, 8)\n",
            "xb: (32, 8), yb: (32, 8)\n",
            "decoded x: she's at ,y: he's at \n",
            "96 µs ± 3.83 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding Tables\n",
        "\n",
        "An embedding table is a table of size `(vocab_size, emb_size)`. Each row is a vector of size `emb_size` for a token in the vocabulary.\n",
        "\n",
        "- Input: `(...)` of integers in the range `0..vocab_size-1`.\n",
        "- Output: `(..., emb_size)`."
      ],
      "metadata": {
        "id": "LyYbMatuFPSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Embed:\n",
        "  vocab_size: int\n",
        "  emb_size: int\n",
        "\n",
        "  def init(self, key):\n",
        "    C = jrandom.normal(key, (self.vocab_size, self.emb_size))\n",
        "    return dict(emb_table=C)\n",
        "\n",
        "  def __call__(self, params, x, **kwargs):\n",
        "    return params['emb_table'][x]"
      ],
      "metadata": {
        "id": "Zrv__us3wXR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emb = Embed(vocab_size, 3)\n",
        "p_emb = emb.init(jrandom.PRNGKey(0))\n",
        "x = jnp.array([0,1,2,3])\n",
        "x.shape, emb(p_emb, x).shape"
      ],
      "metadata": {
        "id": "xGeCvgMwS_rE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9d7cba9-6664-4bf8-dccf-ae3431832b94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4,), (4, 3))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dense Layers\n",
        "\n",
        "A dense layer maps the input (linearly) into a different dimension using `weights` and `bias`.\n",
        "\n",
        "- Input:  (..., fan_in)\n",
        "- Output:  (..., fan_out)\n",
        "- Params:\n",
        "  - Weights: (fan_in, fan_out)\n",
        "  - Bias: (fan_out)\n",
        "\n",
        "This is a generalized form of matrix multiplication, since the input can be more than two dimensions.\n",
        "\n",
        "For example:"
      ],
      "metadata": {
        "id": "BdX-ENfmGOFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Dense:\n",
        "  fan_in: int\n",
        "  fan_out: int\n",
        "  bias: bool = True\n",
        "\n",
        "  def init(self, key):\n",
        "    initializer = nn.initializers.lecun_uniform()\n",
        "    weight = initializer(key, (self.fan_in, self.fan_out))\n",
        "    bias = jnp.zeros((self.fan_out,)) if self.bias else None\n",
        "    return dict(w=weight, b=bias)\n",
        "\n",
        "  def __call__(self, params, x, **kwargs):\n",
        "    out = x @ params['w']\n",
        "    if self.bias:\n",
        "      out += params['b']\n",
        "    return out"
      ],
      "metadata": {
        "id": "g4ilpuvcyJpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dense = Dense(fan_in=3, fan_out=4)\n",
        "p_dense = dense.init(jrandom.PRNGKey(0))\n",
        "x = jnp.array([[1,2,3],[2,3,4]], dtype=jnp.float32)\n",
        "x.shape, dense(p_dense, x).shape"
      ],
      "metadata": {
        "id": "CgMq10YsTC6B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aeb3d747-be26-493b-8b69-4b256b521b5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2, 3), (2, 4))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Baseline: Bigram Model\n",
        "\n",
        "Reproduce the baseline [bigram model](https://youtu.be/kCc8FmEb1nY?t=1337):"
      ],
      "metadata": {
        "id": "mYKbpIoO0aVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class BigramLM:\n",
        "  vocab_size: int\n",
        "  emb_size: int\n",
        "\n",
        "  def __post_init__(self):\n",
        "    self.emb = Embed(self.vocab_size, self.emb_size)\n",
        "    self.net = Dense(self.emb_size, self.vocab_size)\n",
        "\n",
        "    self.layers = {\n",
        "        'emb': self.emb,\n",
        "        'net': self.net\n",
        "    }\n",
        "\n",
        "  def init(self, key):\n",
        "    keys = jrandom.split(key, len(self.layers))\n",
        "    params = {}\n",
        "\n",
        "    for key, name, layer in zip(keys,\n",
        "                                self.layers.keys(),\n",
        "                                self.layers.values()):\n",
        "      params[name] = layer.init(key)\n",
        "    return params\n",
        "\n",
        "  def __call__(self, params, x, **kwargs):\n",
        "    x = self.emb(params['emb'], x, **kwargs)\n",
        "    x = self.net(params['net'], x, **kwargs)\n",
        "    return x\n",
        "\n",
        "  def __hash__(self):\n",
        "    return hash(id(self))\n",
        "\n",
        "  def __eq__(self, other):\n",
        "    return id(self) == id(other)\n",
        "\n",
        "\n",
        "@partial(jit, static_argnames=['model', 'training'])\n",
        "def loss_fn(params, model, x, y, training, key=None):\n",
        "  logits = model(params, x, key, training)\n",
        "  loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=y).mean()\n",
        "  return loss\n",
        "\n",
        "\n",
        "@partial(jit, static_argnames=['model', 'optimizer'])\n",
        "def step_fn(params, model, opt_state, optimizer, x, y, key):\n",
        "  print('first step')\n",
        "  loss, grads = value_and_grad(loss_fn)(params, model, x, y, training=True, key=key)\n",
        "  updates, opt_state = optimizer.update(grads, opt_state, params=params)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "  return loss, params, opt_state\n",
        "\n",
        "\n",
        "def generate(key, params, model, prefix, max_steps, context_length):\n",
        "  result = []\n",
        "  x = jnp.array(encode(prefix))\n",
        "\n",
        "  for step in range(max_steps):\n",
        "    step_key = jrandom.fold_in(key, step)\n",
        "    # truncate x by context length\n",
        "    x = x[-context_length:]\n",
        "    # get last token logits\n",
        "    logits = model(params, x, training=False)[-1]\n",
        "    # sample next token\n",
        "    next_token = jrandom.categorical(step_key, logits=logits)\n",
        "    result.append(next_token.item())\n",
        "    # append next token to input\n",
        "    x = jnp.concatenate((x, jnp.array([next_token])))\n",
        "\n",
        "  return decode(result)\n",
        "\n",
        "\n",
        "def evaluate(key, params, model, context_length, steps=200):\n",
        "  metrics = {}\n",
        "\n",
        "  for split in ['train', 'val']:\n",
        "    loss = []\n",
        "    for step in range(steps):\n",
        "      step_key = jrandom.fold_in(key, step)\n",
        "      xb, yb = get_batch(step_key, split, context_length=context_length)\n",
        "      l = loss_fn(params, model, xb, yb, training=False)\n",
        "      loss.append(l)\n",
        "    metrics[f'{split}_loss'] = jnp.array(loss).mean().item()\n",
        "\n",
        "  return metrics\n",
        "\n",
        "\n",
        "def train_loop(model, total_steps: int,\n",
        "               context_length: int = 8,\n",
        "               batch_size: int = 32,\n",
        "               lr: float=1e-3):\n",
        "  optimizer = optax.adamw(lr)\n",
        "\n",
        "  # init\n",
        "  root_key = jrandom.PRNGKey(0)\n",
        "  model_key, train_key, sample_key, eval_key = jrandom.split(root_key, 4)\n",
        "  params = device_put(model.init(model_key))\n",
        "  opt_state = device_put(optimizer.init(params))\n",
        "\n",
        "  # workaround: vmap currently doesn't support named arguments\n",
        "  def model_apply(params, x, key, training: bool):\n",
        "    return model(params, x, key=key, training=training)\n",
        "  batch_model = vmap(model_apply, in_axes=(None, 0, None, None))\n",
        "\n",
        "  # train\n",
        "  metrics = []\n",
        "  for step in range(total_steps):\n",
        "    step_key = jrandom.fold_in(train_key, step)\n",
        "    xb, yb = get_batch(step_key, 'train',\n",
        "                       batch_size=batch_size,\n",
        "                       context_length=context_length)\n",
        "    step_key, _ = jrandom.split(step_key)\n",
        "    loss, params, opt_state = step_fn(\n",
        "        params, batch_model,\n",
        "        opt_state, optimizer,\n",
        "        xb, yb, step_key)\n",
        "\n",
        "    if step % (total_steps // 10) == 0 or step == total_steps - 1:\n",
        "      # evaluate\n",
        "      metric = evaluate(jrandom.fold_in(eval_key, step),\n",
        "                        params, batch_model, context_length)\n",
        "      metrics.append(metric)\n",
        "      print(f'--- evaluation {step=} ---')\n",
        "      print(f'mini batch loss: {loss.item()}\\n'\n",
        "            f'metrics: {metric}')\n",
        "\n",
        "      print('--- end evaluation ---')\n",
        "\n",
        "  # final sample\n",
        "  sample_result = generate(sample_key,\n",
        "                           params, model,\n",
        "                           prefix='\\n',\n",
        "                           max_steps=500,\n",
        "                           context_length=context_length)\n",
        "  print('Final sample:', sample_result)\n",
        "  return params, metrics"
      ],
      "metadata": {
        "id": "gFsAJzw2DdZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "training bigram model for 10000 steps should give a loss of roughly 2.50"
      ],
      "metadata": {
        "id": "4HCp4FjuJRIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_model = BigramLM(vocab_size, vocab_size)\n",
        "bigram_params, bigram_metrics = train_loop(\n",
        "    bigram_model, total_steps=10000, context_length=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcfrYX8oJO7F",
        "outputId": "be5e7024-3acd-4510-c176-b12c602b0d40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first step\n",
            "--- evaluation step=0 ---\n",
            "mini batch loss: 4.765635967254639\n",
            "metrics: {'train_loss': 4.712054252624512, 'val_loss': 4.699409484863281}\n",
            "--- end evaluation ---\n",
            "--- evaluation step=1000 ---\n",
            "mini batch loss: 2.742424488067627\n",
            "metrics: {'train_loss': 2.635215997695923, 'val_loss': 2.657052755355835}\n",
            "--- end evaluation ---\n",
            "--- evaluation step=2000 ---\n",
            "mini batch loss: 2.530101776123047\n",
            "metrics: {'train_loss': 2.5586252212524414, 'val_loss': 2.606309652328491}\n",
            "--- end evaluation ---\n",
            "--- evaluation step=3000 ---\n",
            "mini batch loss: 2.243786334991455\n",
            "metrics: {'train_loss': 2.508502244949341, 'val_loss': 2.5393943786621094}\n",
            "--- end evaluation ---\n",
            "--- evaluation step=4000 ---\n",
            "mini batch loss: 2.6591711044311523\n",
            "metrics: {'train_loss': 2.513347625732422, 'val_loss': 2.540261745452881}\n",
            "--- end evaluation ---\n",
            "--- evaluation step=5000 ---\n",
            "mini batch loss: 2.180464744567871\n",
            "metrics: {'train_loss': 2.48012638092041, 'val_loss': 2.5483450889587402}\n",
            "--- end evaluation ---\n",
            "--- evaluation step=6000 ---\n",
            "mini batch loss: 2.611126184463501\n",
            "metrics: {'train_loss': 2.4999704360961914, 'val_loss': 2.503690719604492}\n",
            "--- end evaluation ---\n",
            "--- evaluation step=7000 ---\n",
            "mini batch loss: 2.5699210166931152\n",
            "metrics: {'train_loss': 2.5017287731170654, 'val_loss': 2.520076036453247}\n",
            "--- end evaluation ---\n",
            "--- evaluation step=8000 ---\n",
            "mini batch loss: 2.6787352561950684\n",
            "metrics: {'train_loss': 2.49395751953125, 'val_loss': 2.4981493949890137}\n",
            "--- end evaluation ---\n",
            "--- evaluation step=9000 ---\n",
            "mini batch loss: 2.3774499893188477\n",
            "metrics: {'train_loss': 2.4943182468414307, 'val_loss': 2.527510404586792}\n",
            "--- end evaluation ---\n",
            "--- evaluation step=9999 ---\n",
            "mini batch loss: 2.3447186946868896\n",
            "metrics: {'train_loss': 2.4814400672912598, 'val_loss': 2.5175700187683105}\n",
            "--- end evaluation ---\n",
            "Final sample: Thel ppanthethasonncave gweroubetu, WAersofer Gir'd ll s toind;\n",
            "Wes cth whetr\n",
            "O his isatheis!\n",
            "TEORERofor d!\n",
            "PmeeswinousUTh m,\n",
            "RIUSh be s, tu thame y ienilot m l t lll.\n",
            "ORI houtheleery n t momar ppes.\n",
            "CEat inl hithyofe se, soucanonqurto fasthanone thave ndithacke cerace, t t sira pechourse hes bur aklthialighep omyoof ys,\n",
            "O fofu io t her, s cout ' hal th be, ar,\n",
            "Pouns; l of hous\n",
            "BUS:\n",
            "\n",
            "ADe se, cacit d takepevendithed.\n",
            "\n",
            "Thereasol avaventharifonomy beins w?\n",
            "\n",
            "\n",
            "Whe toral  heealol w y pule womeve ais h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dropout\n",
        "\n",
        "Dropout is added at the following places:\n",
        "- the end of every residual path of self attention and feedfoward\n",
        "- the attention weights"
      ],
      "metadata": {
        "id": "8OlGz6t-rnwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Dropout:\n",
        "  rate: float\n",
        "\n",
        "  def init(self, key):\n",
        "    return {}\n",
        "\n",
        "  def __call__(self, x, key=None, training=False, **kwargs):\n",
        "    if training:\n",
        "      mask = jrandom.bernoulli(key, p=(1-self.rate), shape=x.shape)\n",
        "      x = x * mask\n",
        "    else:\n",
        "      x = x / (1-self.rate)\n",
        "    return x"
      ],
      "metadata": {
        "id": "jx9rqnCwrpf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = jrandom.normal(jrandom.PRNGKey(1), (5,5))\n",
        "dropout = Dropout(0.1)\n",
        "out1 = dropout(x, jrandom.PRNGKey(2), training=True)\n",
        "out2 = dropout(x)\n",
        "\n",
        "print(x)\n",
        "print(out1)\n",
        "print(out2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4c2wREwtKMr",
        "outputId": "f0e41661-d93d-4295-84b2-3582d6f53295"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.59333676 -0.82349354  1.1586576   0.61708856  0.5213631 ]\n",
            " [ 0.2781005  -1.2627544   0.05730288 -0.49172685 -0.35850936]\n",
            " [-1.0447503   0.1234699   1.1976635  -0.14236492 -3.7156198 ]\n",
            " [-1.6393571   0.92326057 -1.8844254  -0.96750796 -0.63999134]\n",
            " [ 0.8939773  -0.32139128 -1.1945074   2.2471828  -2.0013103 ]]\n",
            "[[ 0.59333676 -0.82349354  1.1586576   0.61708856  0.        ]\n",
            " [ 0.2781005  -1.2627544   0.05730288 -0.49172685 -0.35850936]\n",
            " [-1.0447503   0.          1.1976635  -0.14236492 -3.7156198 ]\n",
            " [-1.6393571   0.92326057 -1.8844254   0.         -0.63999134]\n",
            " [ 0.8939773  -0.32139128 -1.1945074   2.2471828   0.        ]]\n",
            "[[ 0.6592631  -0.91499287  1.2873974   0.685654    0.57929236]\n",
            " [ 0.30900055 -1.4030606   0.06366987 -0.5463632  -0.39834374]\n",
            " [-1.1608337   0.13718878  1.3307374  -0.15818325 -4.1284666 ]\n",
            " [-1.8215079   1.0258452  -2.093806   -1.0750089  -0.71110153]\n",
            " [ 0.9933081  -0.35710144 -1.3272305   2.49687    -2.2236784 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Self Attention"
      ],
      "metadata": {
        "id": "F4W6RdwbG7de"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: compute bow by averaging previous steps\n",
        "\n",
        "With an input x of `(T, C)`, compute xbow (bag of words), where each step `t` of xbow is the average of x values for steps `1..t-1`."
      ],
      "metadata": {
        "id": "30sgMRjlIn_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (T, C)\n",
        "T, C = 8, 2\n",
        "x = jnp.arange(T * C, dtype=jnp.float32).reshape(T, C)"
      ],
      "metadata": {
        "id": "83b3acDsG-Y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRzq408wIXnu",
        "outputId": "fc68a1d6-724b-4635-ed88-eeb0046aa696"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Array([[ 0.,  1.],\n",
              "        [ 2.,  3.],\n",
              "        [ 4.,  5.],\n",
              "        [ 6.,  7.],\n",
              "        [ 8.,  9.],\n",
              "        [10., 11.],\n",
              "        [12., 13.],\n",
              "        [14., 15.]], dtype=float32),\n",
              " (8, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute with for loop"
      ],
      "metadata": {
        "id": "fkVh1T65I_X1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# use numpy, because jnp arrays are immuatble\n",
        "xbow = np.zeros((T, C))\n",
        "for t in range(T):\n",
        "  xprev = x[:t+1]\n",
        "  xbow[t] = np.mean(xprev, axis=0)\n",
        "xbow, xbow.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dCIAAukHk4F",
        "outputId": "ffe84eaf-15e6-4032-c351-7c40bd430139"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.        , 1.        ],\n",
              "        [1.        , 2.        ],\n",
              "        [2.        , 3.        ],\n",
              "        [3.        , 4.        ],\n",
              "        [4.        , 5.        ],\n",
              "        [5.        , 6.        ],\n",
              "        [6.00000048, 7.00000048],\n",
              "        [7.        , 8.        ]]),\n",
              " (8, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2 implement bow using a weight matrix"
      ],
      "metadata": {
        "id": "wQqZ_ElAJrEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (T, T)\n",
        "weights = jnp.tril(jnp.ones((T, T)))\n",
        "weights = weights / weights.sum(axis=1, keepdims=True)\n",
        "print('weights:\\n', weights)\n",
        "xbow = weights @ x\n",
        "xbow, xbow.shape"
      ],
      "metadata": {
        "id": "crwJ_mUCAGnJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f12eee3-9712-4984-9d4c-b90174a42322"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weights:\n",
            " [[1.         0.         0.         0.         0.         0.\n",
            "  0.         0.        ]\n",
            " [0.5        0.5        0.         0.         0.         0.\n",
            "  0.         0.        ]\n",
            " [0.33333334 0.33333334 0.33333334 0.         0.         0.\n",
            "  0.         0.        ]\n",
            " [0.25       0.25       0.25       0.25       0.         0.\n",
            "  0.         0.        ]\n",
            " [0.2        0.2        0.2        0.2        0.2        0.\n",
            "  0.         0.        ]\n",
            " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667\n",
            "  0.         0.        ]\n",
            " [0.14285715 0.14285715 0.14285715 0.14285715 0.14285715 0.14285715\n",
            "  0.14285715 0.        ]\n",
            " [0.125      0.125      0.125      0.125      0.125      0.125\n",
            "  0.125      0.125     ]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Array([[0.       , 1.       ],\n",
              "        [1.       , 2.       ],\n",
              "        [2.       , 3.       ],\n",
              "        [3.       , 4.       ],\n",
              "        [4.       , 5.       ],\n",
              "        [5.       , 6.0000005],\n",
              "        [6.0000005, 7.0000005],\n",
              "        [7.       , 8.       ]], dtype=float32),\n",
              " (8, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3 implement bow use softmax + mask\n",
        "\n",
        "This seems redundant compared to step 2, however an advantage is that in this approach the initial weights can be any real number rather than a proper probability distribution."
      ],
      "metadata": {
        "id": "5uul2WsfK0fl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask = jnp.tril(jnp.ones((T, T)))\n",
        "weights = jnp.zeros((T, T))\n",
        "weights = jnp.where(mask, weights, float('-inf'))\n",
        "print('masked weights:\\n', weights)\n",
        "weights = nn.softmax(weights, axis=-1)\n",
        "print('normalized weights\\n', weights)\n",
        "xbow = weights @ x\n",
        "xbow, xbow.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKGtr_JpJ6Et",
        "outputId": "e39a5002-6f9c-4f5b-c59c-6e20bb7fa9c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "masked weights:\n",
            " [[  0. -inf -inf -inf -inf -inf -inf -inf]\n",
            " [  0.   0. -inf -inf -inf -inf -inf -inf]\n",
            " [  0.   0.   0. -inf -inf -inf -inf -inf]\n",
            " [  0.   0.   0.   0. -inf -inf -inf -inf]\n",
            " [  0.   0.   0.   0.   0. -inf -inf -inf]\n",
            " [  0.   0.   0.   0.   0.   0. -inf -inf]\n",
            " [  0.   0.   0.   0.   0.   0.   0. -inf]\n",
            " [  0.   0.   0.   0.   0.   0.   0.   0.]]\n",
            "normalized weights\n",
            " [[1.         0.         0.         0.         0.         0.\n",
            "  0.         0.        ]\n",
            " [0.5        0.5        0.         0.         0.         0.\n",
            "  0.         0.        ]\n",
            " [0.33333334 0.33333334 0.33333334 0.         0.         0.\n",
            "  0.         0.        ]\n",
            " [0.25       0.25       0.25       0.25       0.         0.\n",
            "  0.         0.        ]\n",
            " [0.2        0.2        0.2        0.2        0.2        0.\n",
            "  0.         0.        ]\n",
            " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667\n",
            "  0.         0.        ]\n",
            " [0.14285715 0.14285715 0.14285715 0.14285715 0.14285715 0.14285715\n",
            "  0.14285715 0.        ]\n",
            " [0.125      0.125      0.125      0.125      0.125      0.125\n",
            "  0.125      0.125     ]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Array([[0.       , 1.       ],\n",
              "        [1.       , 2.       ],\n",
              "        [2.       , 3.       ],\n",
              "        [3.       , 4.       ],\n",
              "        [4.       , 5.       ],\n",
              "        [5.       , 6.0000005],\n",
              "        [6.0000005, 7.0000005],\n",
              "        [7.       , 8.       ]], dtype=float32),\n",
              " (8, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: self-attention\n",
        "\n",
        "1. project x into 3 vectors `q, k, v`, of shape `(T, C)`\n",
        "2. weights is obtained by `q @ k.T` shape: `(T, T)`\n",
        "3. output is obtained by `weights @ v` shape: `(T, C)`."
      ],
      "metadata": {
        "id": "wLKszphiVKlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "T, C = 8, 32\n",
        "head_size = 16\n",
        "\n",
        "key_x, key_k, key_q, key_v = jrandom.split(jrandom.PRNGKey(0), 4)\n",
        "x = jrandom.normal(key_x, (T, C))\n",
        "\n",
        "key = Dense(C, head_size, bias=False)\n",
        "query = Dense(C, head_size, bias=False)\n",
        "value = Dense(C, head_size, bias=False)\n",
        "p_k = key.init(key_k)\n",
        "p_q = query.init(key_q)\n",
        "p_v = value.init(key_v)\n",
        "\n",
        "q, k, v = query(p_q, x), key(p_k, x), value(p_v, x)\n",
        "\n",
        "print(q.shape, k.shape, v.shape)\n",
        "print('v:\\n', v)\n",
        "\n",
        "weights = q @ k.T\n",
        "print('raw weights:\\n', weights)\n",
        "\n",
        "mask = jnp.tril(jnp.ones((T, T)))\n",
        "weights = jnp.where(mask, weights, float('-inf'))\n",
        "print('masked weights:\\n', weights)\n",
        "weights = nn.softmax(weights, axis=-1)\n",
        "print('normalized weights\\n', weights)\n",
        "out = weights @ v\n",
        "out, out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqU5bRilVOfT",
        "outputId": "94c6a4e1-f3d4-4384-f64f-efd67c446c23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8, 16) (8, 16) (8, 16)\n",
            "v:\n",
            " [[ 0.24330884  0.912989   -0.05216545  0.01312336 -0.03493929 -0.67756546\n",
            "  -0.012977    0.50382483  0.43943962  1.9789965   0.39075887 -0.035795\n",
            "  -0.3010243  -1.8513116   0.4575368  -0.937912  ]\n",
            " [ 0.47384062  0.53008753  0.7318371  -1.3311323  -0.0116291  -1.1984854\n",
            "   0.8337784  -1.0575992  -1.2903666   0.42020324  0.4199374   0.44925377\n",
            "   0.30578464 -0.9753333  -0.5892321  -0.17671284]\n",
            " [ 0.31038776  1.0070648  -0.55125964 -0.7630973  -0.9848412   0.12845135\n",
            "   0.5860515  -1.2459748   0.6291082  -1.2544262   1.5047724  -0.600275\n",
            "   0.53391063 -0.7571485  -0.6518285   0.5858216 ]\n",
            " [ 0.49194252  0.08106694 -1.1507456   1.0922581  -0.04301414  0.03573557\n",
            "   0.6306269   0.38372898 -0.11793888  0.9654348  -0.1945737  -1.2260118\n",
            "   0.04184315  0.30720368  0.11399549 -0.42123383]\n",
            " [ 0.9469206  -1.569232    0.5465861  -2.290162   -0.954916    0.5616087\n",
            "  -0.8563687   1.8234979   0.01479638 -0.99293065  0.43147808  1.7037365\n",
            "   0.840355   -0.84829473 -0.747861    1.0664277 ]\n",
            " [ 1.6956844  -0.11534239 -1.199537   -0.08711138  0.13695055 -1.3188653\n",
            "  -0.47669256  1.0186015  -0.96485376 -0.29131424  0.1802854  -0.15312868\n",
            "   1.0196195  -1.4811044  -0.79046    -2.3772593 ]\n",
            " [-0.09189808  0.8924726   0.7829564   0.4982675   0.40117612 -0.06166852\n",
            "   0.40300483  1.882246    1.5449111  -0.07689744  1.6187903  -1.2706587\n",
            "   0.6963316   0.5413357  -1.3530843  -1.2535295 ]\n",
            " [-0.0236129   0.36506453  0.705116   -1.4806137  -0.5162303  -0.4046365\n",
            "   1.0652126  -1.0577483  -0.7451378  -0.5706383   1.8458076   0.515451\n",
            "  -0.6588458  -0.28666526  0.47856635  0.28629684]]\n",
            "raw weights:\n",
            " [[  2.441443    -0.9126426   -0.16153085  -2.2524822   -1.8852967\n",
            "   -0.47920895   2.6829202    0.05522335]\n",
            " [ -6.3328533   -4.8715577    3.9452403   -1.8688698    1.9682579\n",
            "    2.1484756    2.2096958   -5.926384  ]\n",
            " [ -2.5939102    8.940504    -4.4174337    9.739806     1.2640417\n",
            "   -0.8116516   -6.1588       2.777226  ]\n",
            " [ 10.034026   -10.121895    -1.3382523   -1.9290345   -0.43742776\n",
            "    1.9217516    8.414541    -2.2159047 ]\n",
            " [ -2.7413313    8.008858    -4.5563555    2.287939    -2.505909\n",
            "   -2.7744846   -3.926293     3.3465173 ]\n",
            " [  2.4515986   -2.6243255    2.479877    -6.965022    -0.92782754\n",
            "    1.4243177    5.2777653    0.59847116]\n",
            " [  2.4854465    5.345042     0.23339605  -3.6697712   -2.8825538\n",
            "   -0.78208673   1.97867      5.332842  ]\n",
            " [ -1.2250887   -2.0607347   -0.7713232   -4.3581753    3.203529\n",
            "    1.9144485    5.8811836    1.2815868 ]]\n",
            "masked weights:\n",
            " [[  2.441443           -inf         -inf         -inf         -inf\n",
            "          -inf         -inf         -inf]\n",
            " [ -6.3328533   -4.8715577          -inf         -inf         -inf\n",
            "          -inf         -inf         -inf]\n",
            " [ -2.5939102    8.940504    -4.4174337          -inf         -inf\n",
            "          -inf         -inf         -inf]\n",
            " [ 10.034026   -10.121895    -1.3382523   -1.9290345          -inf\n",
            "          -inf         -inf         -inf]\n",
            " [ -2.7413313    8.008858    -4.5563555    2.287939    -2.505909\n",
            "          -inf         -inf         -inf]\n",
            " [  2.4515986   -2.6243255    2.479877    -6.965022    -0.92782754\n",
            "    1.4243177          -inf         -inf]\n",
            " [  2.4854465    5.345042     0.23339605  -3.6697712   -2.8825538\n",
            "   -0.78208673   1.97867            -inf]\n",
            " [ -1.2250887   -2.0607347   -0.7713232   -4.3581753    3.203529\n",
            "    1.9144485    5.8811836    1.2815868 ]]\n",
            "normalized weights\n",
            " [[1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
            "  0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
            " [1.8826924e-01 8.1173074e-01 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
            "  0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
            " [9.7872935e-06 9.9998868e-01 1.5802159e-06 0.0000000e+00 0.0000000e+00\n",
            "  0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
            " [9.9998212e-01 1.7635450e-09 1.1509980e-05 6.3753073e-06 0.0000000e+00\n",
            "  0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
            " [2.1370222e-05 9.9668223e-01 3.4797974e-06 3.2658281e-03 2.7042795e-05\n",
            "  0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
            " [4.1202208e-01 2.5732073e-03 4.2383969e-01 3.3522334e-05 1.4036355e-02\n",
            "  1.4749524e-01 0.0000000e+00 0.0000000e+00]\n",
            " [5.2064423e-02 9.0875691e-01 5.4763095e-03 1.1050044e-04 2.4279967e-04\n",
            "  1.9836698e-03 3.1365402e-02 0.0000000e+00]\n",
            " [7.4525818e-04 3.2313965e-04 1.1732068e-03 3.2480635e-05 6.2464155e-02\n",
            "  1.7210377e-02 9.0891141e-01 9.1399113e-03]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Array([[ 0.24330884,  0.912989  , -0.05216545,  0.01312336, -0.03493929,\n",
              "         -0.67756546, -0.012977  ,  0.50382483,  0.43943962,  1.9789965 ,\n",
              "          0.39075887, -0.035795  , -0.3010243 , -1.8513116 ,  0.4575368 ,\n",
              "         -0.937912  ],\n",
              "        [ 0.43043858,  0.60217613,  0.5842335 , -1.0780503 , -0.01601769,\n",
              "         -1.1004121 ,  0.6743604 , -0.76363105, -0.9646973 ,  0.71367604,\n",
              "          0.41444397,  0.357934  ,  0.19154118, -1.1402531 , -0.3921577 ,\n",
              "         -0.32002324],\n",
              "        [ 0.47383812,  0.53009206,  0.7318274 , -1.3311182 , -0.01163087,\n",
              "         -1.1984782 ,  0.83376974, -1.0575843 , -1.2903467 ,  0.42021585,\n",
              "          0.41993886,  0.4492474 ,  0.30577907, -0.97534156, -0.58922195,\n",
              "         -0.1767191 ],\n",
              "        [ 0.2433112 ,  0.9129848 , -0.0521782 ,  0.01312131, -0.03495027,\n",
              "         -0.6775516 , -0.012966  ,  0.5038039 ,  0.43943825,  1.9789529 ,\n",
              "          0.39076796, -0.03580909, -0.3010125 , -1.8512852 ,  0.4575218 ,\n",
              "         -0.9378912 ],\n",
              "        [ 0.47390702,  0.52857417,  0.72566265, -1.3232131 , -0.011761  ,\n",
              "         -1.1943913 ,  0.8330502 , -1.0527812 , -1.2864587 ,  0.42197314,\n",
              "          0.41793397,  0.44380254,  0.3049249 , -0.9711592 , -0.5869176 ,\n",
              "         -0.1774914 ],\n",
              "        [ 0.49643576,  0.76533365, -0.42254835, -0.3664065 , -0.4250459 ,\n",
              "         -0.41445535,  0.16288152, -0.14738102,  0.30227184,  0.22792372,\n",
              "          0.8325052 , -0.26672527,  0.26523665, -1.3165531 , -0.21635427,\n",
              "         -0.47428054],\n",
              "        [ 0.44573897,  0.56216186,  0.68151104, -1.1981512 , -0.00516231,\n",
              "         -1.1281157 ,  0.77179223, -0.8801495 , -1.099772  ,  0.47490412,\n",
              "          0.46142116,  0.36323124,  0.28920707, -0.9730055 , -0.559394  ,\n",
              "         -0.2500335 ],\n",
              "        [ 0.0053035 ,  0.7165452 ,  0.73109376,  0.29351592,  0.3014376 ,\n",
              "         -0.04810769,  0.31530303,  1.8311453 ,  1.3823403 , -0.14197445,\n",
              "          1.5204482 , -1.0470436 ,  0.6974243 ,  0.40835452, -1.2863889 ,\n",
              "         -1.1111131 ]], dtype=float32),\n",
              " (8, 16))"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Multi Head Attention\n",
        "\n",
        "A multi head attention is just splitting the projection matrix `Q, K, V` into n heads, do attention on each of the heads, then concatenate the results together.\n",
        "\n",
        "A naive implementation would be:\n",
        "\n",
        "```python\n",
        "def multi_head_attn(x, dim, n_heads):\n",
        "  head_size = dim / n_heads\n",
        "  # each head projects x from `dim` into q, k, v of `head_size`\n",
        "  heads = [self_attn(dim, head_size) for _ in range(n_heads)]\n",
        "  return cat([h(x) for h in heads], dim=-1)\n",
        "```\n",
        "\n",
        "But they can also be implemented more efficiently by using matrix multiplication, avoiding the for loop:"
      ],
      "metadata": {
        "id": "rLmZMqaIid1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class SelfAttentionHead:\n",
        "  dim: int\n",
        "  num_heads: int\n",
        "  context_length: int\n",
        "  dropout: float = 0.0\n",
        "\n",
        "  def __post_init__(self):\n",
        "    self.key = Dense(self.dim, self.dim, bias=False)\n",
        "    self.query = Dense(self.dim, self.dim, bias=False)\n",
        "    self.value = Dense(self.dim, self.dim, bias=False)\n",
        "\n",
        "    self.output = Dense(self.dim, self.dim, bias=True)\n",
        "\n",
        "    self.dropout_weights = Dropout(self.dropout)\n",
        "\n",
        "    # constant\n",
        "    self.mask = jnp.tril(jnp.ones((self.context_length, self.context_length)))\n",
        "\n",
        "  def init(self, key):\n",
        "    key_k, key_q, key_v, key_o = jrandom.split(jrandom.PRNGKey(0), 4)\n",
        "    params = {\n",
        "        'key': self.key.init(key_k),\n",
        "        'query': self.query.init(key_q),\n",
        "        'value': self.value.init(key_v),\n",
        "        'output': self.output.init(key_o),\n",
        "    }\n",
        "    return params\n",
        "\n",
        "  def __call__(self, params, x, **kwargs):\n",
        "    T, C = x.shape\n",
        "    head_dim = self.dim // self.num_heads\n",
        "\n",
        "    q = self.query(params['query'], x, **kwargs).reshape(-1, self.num_heads, head_dim)\n",
        "    k = self.key(params['key'], x, **kwargs).reshape(-1, self.num_heads, head_dim)\n",
        "    v = self.value(params['value'], x, **kwargs).reshape(-1, self.num_heads, head_dim)\n",
        "\n",
        "    weights = jnp.einsum('qhd,khd->hqk', q, k)\n",
        "    weights = weights * self.dim ** -0.5\n",
        "    weights = jnp.where(self.mask[:T, :T], weights, float('-inf'))\n",
        "    weights = nn.softmax(weights, axis=-1)\n",
        "    weights = self.dropout_weights(weights, **kwargs)\n",
        "\n",
        "    out = jnp.einsum('hqk,khd->qhd', weights, v)\n",
        "    out = out.reshape(-1, self.dim)\n",
        "    out = self.output(params['output'], out, **kwargs)\n",
        "    return out\n",
        "\n"
      ],
      "metadata": {
        "id": "BFTYgNERihFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "T, C = 8, 16\n",
        "head_size = 16\n",
        "\n",
        "x = jrandom.normal(jrandom.PRNGKey(0), (T, C))\n",
        "\n",
        "head = SelfAttentionHead(head_size, 2, T)\n",
        "params = head.init(jrandom.PRNGKey(0))\n",
        "out = head(params, x)\n",
        "\n",
        "print('input:', x.shape, 'output:', out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9fI8a7LvjPa",
        "outputId": "14be1747-23bf-4232-a3c1-ea9f7bfabf39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input: (8, 16) output: (8, 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feed Forward Network\n",
        "\n",
        "This is just a simple MLP."
      ],
      "metadata": {
        "id": "Ta0fScp-AOOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class FeedFoward:\n",
        "  input_size: int\n",
        "  hidden_size: int\n",
        "\n",
        "  def __post_init__(self):\n",
        "    self.dense1 = Dense(self.input_size, self.hidden_size)\n",
        "    self.dense2 = Dense(self.hidden_size, self.input_size)\n",
        "\n",
        "  def init(self, key):\n",
        "    keys = jrandom.split(key, 2)\n",
        "    params = {\n",
        "        'dense1': self.dense1.init(keys[0]),\n",
        "        'dense2': self.dense2.init(keys[1]),\n",
        "    }\n",
        "    return params\n",
        "\n",
        "  def __call__(self, params, x, **kwargs):\n",
        "    x = self.dense1(params['dense1'], x)\n",
        "    x = nn.relu(x)\n",
        "    x = self.dense2(params['dense2'], x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "aYtDAYy4AR2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ffn = FeedFoward(32, 64)\n",
        "params = ffn.init(jrandom.PRNGKey(0))\n",
        "x = jrandom.normal(jrandom.PRNGKey(0), (8,32))\n",
        "out = ffn(params, x)\n",
        "print(x.shape, out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04IaZN5dBMnb",
        "outputId": "3449aa76-d477-4dad-ebc6-9a0f54127f9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8, 32) (8, 32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LayerNorm\n",
        "\n",
        "Layernorm is similar to batchnorm. The only difference is that layernorm is computing mean and variance along the channel axis.\n",
        "\n",
        "Regarding the scaling params, they work the same as BatchNorm."
      ],
      "metadata": {
        "id": "8DH6N6C1dSSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class LayerNorm:\n",
        "  dim: int\n",
        "  eps: float = 1e-5\n",
        "\n",
        "  def init(self, key):\n",
        "    return {\n",
        "        'weight': jnp.ones(self.dim),\n",
        "        'bias': jnp.zeros(self.dim)\n",
        "    }\n",
        "\n",
        "  def __call__(self, params, x, **kwargs):\n",
        "    xmean = jnp.mean(x, axis=-1, keepdims=True)\n",
        "    xvar = jnp.var(x, axis=-1, keepdims=True)\n",
        "    x = (x - xmean) / jnp.sqrt(xvar + self.eps)\n",
        "    x = x * params['weight'] + params['bias']\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "rCLH1NhPdpwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = jrandom.normal(jrandom.PRNGKey(0), (8, 32))\n",
        "ln = LayerNorm(32)\n",
        "p_ln = ln.init(jrandom.PRNGKey(0))\n",
        "out = ln(p_ln, x)\n",
        "print(x.shape, out.shape)\n",
        "print(out[0].mean(), out[0].std())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPe93wh-jEvt",
        "outputId": "d1375885-088b-427a-f032-af3edd0dfab2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8, 32) (8, 32)\n",
            "7.450581e-09 0.99999404\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Block\n",
        "\n",
        "We can combine self-attention and feed foward to a transformer decoder block.\n",
        "\n",
        "Then we can add many blocks to scale the model up."
      ],
      "metadata": {
        "id": "ibfxja0nCfRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Block:\n",
        "  emb_size: int\n",
        "  num_heads: int\n",
        "  context_length: int\n",
        "  dropout: float = 0.0\n",
        "\n",
        "  def __post_init__(self):\n",
        "    self.sa_head = SelfAttentionHead(self.emb_size,\n",
        "                                     self.num_heads,\n",
        "                                     self.context_length,\n",
        "                                     self.dropout)\n",
        "    self.ffn = FeedFoward(self.emb_size, self.emb_size*4)\n",
        "    self.ln1 = LayerNorm(self.emb_size)\n",
        "    self.ln2 = LayerNorm(self.emb_size)\n",
        "    self.dropout1 = Dropout(self.dropout)\n",
        "    self.dropout2 = Dropout(self.dropout)\n",
        "\n",
        "  def init(self, key):\n",
        "    keys = jrandom.split(key, 4)\n",
        "    params = {\n",
        "        'sa_head': self.sa_head.init(keys[0]),\n",
        "        'ffn': self.ffn.init(keys[1]),\n",
        "        'ln1': self.ln1.init(keys[2]),\n",
        "        'ln2': self.ln2.init(keys[3]),\n",
        "    }\n",
        "    return params\n",
        "\n",
        "  def __call__(self, params, x, **kwargs):\n",
        "    x = self.ln1(params['ln1'], x)\n",
        "    x = x + self.dropout1(self.sa_head(params['sa_head'], x))\n",
        "    x = self.ln2(params['ln2'], x)\n",
        "    x = x + self.dropout2(self.ffn(params['ffn'], x))\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "UUwT1E0_CriD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block = Block(emb_size=32, num_heads=4, context_length=8)\n",
        "params = block.init(jrandom.PRNGKey(0))\n",
        "x = jrandom.normal(jrandom.PRNGKey(0), (8,32))\n",
        "out = block(params, x)\n",
        "print(x.shape, out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PemL64NsDgo9",
        "outputId": "00d0955b-74c6-455d-c25d-ed9d438e0c6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8, 32) (8, 32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TransformerStack\n",
        "\n",
        "This is just a stack of multiple Blocks, plus a final layer norm layer."
      ],
      "metadata": {
        "id": "xuZnCQxcFcd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class TransformerStack:\n",
        "  emb_size: int\n",
        "  num_heads: int\n",
        "  num_blocks: int\n",
        "  context_length: int\n",
        "  dropout: float = 0.0\n",
        "\n",
        "  def __post_init__(self):\n",
        "    self.blocks = [Block(self.emb_size,\n",
        "                         self.num_heads,\n",
        "                         self.context_length,\n",
        "                         self.dropout) \\\n",
        "                   for _ in range(self.num_blocks)]\n",
        "    self.ln = LayerNorm(self.emb_size)\n",
        "\n",
        "  def init(self, key):\n",
        "    keys = jrandom.split(key, len(self.blocks)+1)\n",
        "    params = {}\n",
        "\n",
        "    for key, (i, block) in zip(keys[:-1], enumerate(self.blocks)):\n",
        "      params[f'block_{i}'] = block.init(key)\n",
        "\n",
        "    params['final_ln'] = self.ln.init(keys[-1])\n",
        "    return params\n",
        "\n",
        "  def __call__(self, params, x, **kwargs):\n",
        "    for i, block in enumerate(self.blocks):\n",
        "      x = block(params[f'block_{i}'], x)\n",
        "    x = self.ln(params['final_ln'], x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "xrFtQqHlEj_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer_stack = TransformerStack(emb_size=32,\n",
        "                                     num_heads=4,\n",
        "                                     context_length=8,\n",
        "                                     num_blocks=4)\n",
        "\n",
        "params = transformer_stack.init(jrandom.PRNGKey(0))\n",
        "x = jrandom.normal(jrandom.PRNGKey(0), (8,32))\n",
        "out = transformer_stack(params, x)\n",
        "print(x.shape, out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkDyai2QFJ8J",
        "outputId": "807aa940-304e-4a6d-cdff-0208136b6286"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8, 32) (8, 32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement TransformerLM\n",
        "\n",
        "Now we can put all the ideas together:\n",
        "- token Embeddings `(T, emb)`\n",
        "- positional Embeddings  `(T, emb)`\n",
        "- transformer blocks x N  `(T, emb)`\n",
        "  - self attention\n",
        "  - feed foward\n",
        "  - residual\n",
        "  - layernorm\n",
        "  - dropout\n",
        "- language model head  `(T, vocab_size)`"
      ],
      "metadata": {
        "id": "dmhTRTazN07c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class TransformerLM:\n",
        "  vocab_size: int\n",
        "  emb_size: int\n",
        "  context_length: int\n",
        "  num_heads: int\n",
        "  num_blocks: int\n",
        "  dropout: float = 0.0\n",
        "\n",
        "  def __post_init__(self):\n",
        "    self.emb = Embed(self.vocab_size, self.emb_size)\n",
        "    self.pos_emb = Embed(self.context_length, self.emb_size)\n",
        "    self.transformer_stack = TransformerStack(\n",
        "        emb_size=self.emb_size,\n",
        "        num_heads=self.num_heads,\n",
        "        num_blocks=self.num_blocks,\n",
        "        context_length=self.context_length,\n",
        "        dropout=self.dropout)\n",
        "    self.lm_head = Dense(self.emb_size, self.vocab_size)\n",
        "\n",
        "    self.layers = {\n",
        "        'emb': self.emb,\n",
        "        'pos_emb': self.pos_emb,\n",
        "        'transformer_stack': self.transformer_stack,\n",
        "        'lm_head': self.lm_head,\n",
        "    }\n",
        "\n",
        "  def init(self, key):\n",
        "    keys = jrandom.split(key, len(self.layers))\n",
        "    params = {}\n",
        "\n",
        "    for key, name, layer in zip(keys,\n",
        "                                self.layers.keys(),\n",
        "                                self.layers.values()):\n",
        "      params[name] = layer.init(key)\n",
        "    return params\n",
        "\n",
        "  def __call__(self, params, x, **kwargs):\n",
        "    T = x.shape[0]\n",
        "\n",
        "    token_emb = self.emb(params['emb'], x, **kwargs) # (T, emb_size)\n",
        "    pos_emb = self.pos_emb(params['pos_emb'], jnp.arange(T), **kwargs) # (T, emb_size)\n",
        "\n",
        "    x = token_emb + pos_emb # (T, emb_size)\n",
        "    x = self.transformer_stack(params['transformer_stack'], x) # (T, emb_size)\n",
        "\n",
        "    logits = self.lm_head(params['lm_head'], x, **kwargs) # (T, vocab_size)\n",
        "    return logits\n",
        "\n",
        "  def __hash__(self):\n",
        "    return hash(id(self))\n",
        "\n",
        "  def __eq__(self, other):\n",
        "    return id(self) == id(other)\n",
        "\n",
        "\n",
        "model = TransformerLM(\n",
        "    vocab_size=vocab_size,\n",
        "    # emb_size=384,\n",
        "    emb_size=96,\n",
        "    num_heads=6,\n",
        "    num_blocks=6,\n",
        "    context_length=256,\n",
        "    dropout=0.2)\n",
        "params, metrics = train_loop(model,\n",
        "                             total_steps=10000,\n",
        "                             context_length=256,\n",
        "                             batch_size=64,\n",
        "                             lr=3e-4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooLRxMicLbSU",
        "outputId": "98774c59-932e-4e2e-d626-a135be4984a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first step\n",
            "--- evaluation step=0 ---\n",
            "mini batch loss: 4.861026763916016\n",
            "metrics: {'train_loss': 4.228540420532227, 'val_loss': 4.239792823791504}\n",
            "--- end evaluation ---\n",
            "--- evaluation step=1000 ---\n",
            "mini batch loss: 2.1393415927886963\n",
            "metrics: {'train_loss': 2.1552505493164062, 'val_loss': 2.188020944595337}\n",
            "--- end evaluation ---\n",
            "--- evaluation step=2000 ---\n",
            "mini batch loss: 1.7912817001342773\n",
            "metrics: {'train_loss': 1.7964385747909546, 'val_loss': 1.9257885217666626}\n",
            "--- end evaluation ---\n",
            "--- evaluation step=3000 ---\n",
            "mini batch loss: 1.6336050033569336\n",
            "metrics: {'train_loss': 1.6405086517333984, 'val_loss': 1.8075802326202393}\n",
            "--- end evaluation ---\n",
            "--- evaluation step=4000 ---\n",
            "mini batch loss: 1.561478614807129\n",
            "metrics: {'train_loss': 1.5583171844482422, 'val_loss': 1.7364275455474854}\n",
            "--- end evaluation ---\n",
            "--- evaluation step=5000 ---\n",
            "mini batch loss: 1.5019328594207764\n",
            "metrics: {'train_loss': 1.5011539459228516, 'val_loss': 1.697832465171814}\n",
            "--- end evaluation ---\n",
            "--- evaluation step=6000 ---\n",
            "mini batch loss: 1.4700853824615479\n",
            "metrics: {'train_loss': 1.4643561840057373, 'val_loss': 1.6708776950836182}\n",
            "--- end evaluation ---\n",
            "--- evaluation step=7000 ---\n",
            "mini batch loss: 1.4350680112838745\n",
            "metrics: {'train_loss': 1.4392166137695312, 'val_loss': 1.6524511575698853}\n",
            "--- end evaluation ---\n",
            "--- evaluation step=8000 ---\n",
            "mini batch loss: 1.3840088844299316\n",
            "metrics: {'train_loss': 1.4103440046310425, 'val_loss': 1.632015347480774}\n",
            "--- end evaluation ---\n",
            "--- evaluation step=9000 ---\n",
            "mini batch loss: 1.377044677734375\n",
            "metrics: {'train_loss': 1.3942676782608032, 'val_loss': 1.627993106842041}\n",
            "--- end evaluation ---\n",
            "--- evaluation step=9999 ---\n",
            "mini batch loss: 1.4026801586151123\n",
            "metrics: {'train_loss': 1.3719760179519653, 'val_loss': 1.615073323249817}\n",
            "--- end evaluation ---\n",
            "Final sample: JULIET:\n",
            "Had then, none you are the lady so, revilly leasure,\n",
            "When, if I was you have savagined is close.\n",
            "\n",
            "VIRGILIA:\n",
            "Who's all majes they gluckere--\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "What fear'd keep you to omack seeming\n",
            "Methinks his zenumzary.\n",
            "\n",
            "MENENIUS:\n",
            "Pet not:\n",
            "If me not, we are rack, the sir, perhap, his some king.\n",
            "\n",
            "MARCIUS:\n",
            "I'll commended that tell-mistroubl'd allend his\n",
            "Frarius in and house of marre one can time!\n",
            "\n",
            "Speak:\n",
            "There's needs,\n",
            "I pain the staff'd the instand and offenit he\n",
            "altience pule-wored.\n",
            "\n",
            "SAMP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results\n",
        "\n",
        "steps = 10000\n",
        "context_length = 8\n",
        "embed_size = 32\n",
        "learning_rate = 1e-3\n",
        "\n",
        "\n",
        "- Bigram(Baseline) 2.47 / 2.49\n",
        "- Single head: train 2.35 / val 2.37\n",
        "- 2 heads: train 2.24 / val 2.27\n",
        "- 4 heads: 2.19 / 2.27\n",
        "- +FFN: 2.092 / 2.148\n",
        "- 4 blocks:  2.059 / 2.139\n",
        "- +residual connection and projection: 1.932 / 2.047\n",
        "- +layernorm: 1.902 / 2.019\n",
        "- scale up: 6 layers, context length 256, dim = 96, dropout = 0.2: 1.37 / 1.61"
      ],
      "metadata": {
        "id": "FVUrFlDp9ysh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0w1U8fm1OjSY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}